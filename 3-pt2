            mm            
            MM            
`7MMpdMAo.mmMMmm  pd*"*b. 
  MM   `Wb  MM   (O)   j8 
  MM    M8  MM       ,;j9 
  MM   ,AP  MM    ,-='    
  MMbmmd'   `MbmoAmmmmmmm 
  MM                      
.JMML.

Singularity Systems: Zero to Hero

--------------------------------------------------------------------------------
Contents:
  0. au197
  1. dfdx(nd)
  2. brrr
  3. pt2 <-- HERE
    Part 0: DLPL — Deep Learning F̶r̶a̶m̶e̶w̶o̶r̶k̶s̶ Programming Languages
    Part 1: Frontend — Graph Capture
    Part 2: Backend — Optimization & Generation
  4. cloud
Appendix:
  A. c2r5 — Traditional Compiler Construction
  B. trs — Kernel DS(Language)
  C. licurs — 
--------------------------------------------------------------------------------

-------------------------------------------------------------
Part 0: DLPL — Deep Learning F̶r̶a̶m̶e̶w̶o̶r̶k̶s̶ Programming Languages
-------------------------------------------------------------
Can we build systems that treat numerics, derivatives and parallelism as first-class features, without sacrificing traditional programming ideas and wisdom? This is the foundational question which languages over the coming decade will have to answer.

--------------------------------
Part 1: Frontend — Graph Capture
--------------------------------
  1a - foo: torch.jit.trace
  1b — bar: torch.fx.symbolic_trace
  1c — Dynamic Bytecode Transformation: TorchDynamo





References:
productivity-performance-pareto
- DLPL: https://math.mit.edu/~edelman/publications/on_machine_learning.pdf
- graph-graph: TF1/caffe/theano https://arxiv.org/abs/1605.08695, https://dl.acm.org/doi/abs/10.1145/2647868.2654889, https://arxiv.org/abs/1605.02688
- eager-eager: Chainer/PT1/TF2 https://arxiv.org/abs/1908.00213, https://arxiv.org/pdf/1912.01703, https://arxiv.org/abs/1903.01855
- eager-graph (compiler):
  - PT2 https://arxiv.org/abs/2002.03794, https://dl.acm.org/doi/pdf/10.1145/3620665.3640366, https://www.youtube.com/watch?v=WxYEoTLgdLo
  - TVM: https://arxiv.org/abs/1802.04799

frontend: graph capture
1. jit.trace
2. fx.trace? symbolic tracing
- https://mlsys.org/Conferences/doc/2018/146.pdf
3. dynamic bytecode transforms
- https://arxiv.org/abs/2112.08429
- https://pytorch.org/docs/stable/torch.compiler_dynamo_overview.html
- https://pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html
- https://www.youtube.com/watch?v=egZB5Uxki0I
- https://www.youtube.com/watch?v=5FNHwPIyHr8

backend: codegen?













































devils advocate: if we're all running one model, do we need compilers? why not just hand author the optimizations?
b/c the purpose of compilers is generality.
ansel: generality. benchmarks.

--------------------------
Part 0: Tensor Compilation
--------------------------
- https://www.reddit.com/r/Compilers/comments/1d089zz/learning_path_for_ml_compilers/
- https://www.reddit.com/r/Compilers/comments/130mzdy/beginner_projectsresources_to_learn_about_ml/


1. torch.compile
2. flash attention: kernel optimization
3. flex attention: kernel authoring

                programming model        execution model
                ----------------------------------------
Tensorflow 1  |  graph                    graph
PyTorch 1     |  graph                    eager
Modern ML/DL  |  eager                    graph

modern ML/DL: eager programming. graph execution
-> graph captured needed for optimizations
-> optimizations are now needed because post-Pascal (volta, ampere, hopper, blackwell),
    tensor cores make the other 10% very slow???
torchcompile
jax.jit
tinygrad
mlx
                          % flop     %runtime 
tensor contraction        99.80      61.0
stat. normalization       0.17       25.5 <-- why are we letting the slow ops run 25% of the time (due to mem bw bottlenecking)?
element-wise              0.03       13.5 <-- why are we letting the slow ops run 13% of the time (due to mem bw bottlenecking)?


optimizations
- compute: fusion
- memory: recomputation vs reuse (like register spilling)
- overhead: cudagraphs, or codegen lower overhead wrapper

why use a tensor compiler over handwritten optimizations if we're all running one workload (AR/NTP transformers) with $XBs.
what programming model do we use for flash attention? fuse qk = mm, qk_scores = softmax(qk), out = mm (qk_scores, V)
- pattern match?
- decided on F.scaled_dot_product_attention()
-> still frustating. introducing high level instruction isn't flexible
- eg
    - sliding window attention
    - alibi
    - softcapping
    - pagedattention
    - neighborhood attention
    - alibi
    - relative positional encoding
    - jagged sequences
    - prefixlm
    - treeattention
    - transfusion
    - causal
    - graphattention

flash_attn_forward(....) <-- kwargs keep growing

one monolithic function in the programming model / api isn't flexible enough

THEREFORE: flexattention API (it's no surprise nvidia uses flexattention api for llm generation)
-> built on top of torch.compile
-> from in person, pytorch was the right layer of the stack to build something like flexattention
-> programming models > implementations

programming models > optimizations
-> it's not like you can get a group of 5 cracked compiler engineers in a room to write compiler passes so everyone can leverage their optimizations
-> this is b/c the workloads of neural networks (machine learning/deep learning) are constantly in flux. so you need to 
    create the right programming model that is flexible enough to express functions that weren't thought of in the beginning.
-> grothendieck: expressing the problem with the most suitable model
-> story of ispc pharr's blog: the lead ICPC compiler guy was baffled and kept asking
"what happens when the CUDA compiler fails to vectorize", not understanding that it's impossible for that to happen,
since parallelization is *inherent* with the CUDA programming model (ian buck).
p13n is NOT inherent with the autovectorization programming model.

compilation is hard enough.
but now we get into compilation intersect distributed.
next section part 4 - cloud.
jist: for a given set of paralleism schemes, it's feasible to create an analytical model to determine best parallelism strat.
-> problem: new parallelism schemes (and concerns) keep on coming.
the issue of automating paralleism with compilers is that most of the gains does not come from searching within your search sapce,
but expanding your search space, along a new dimension.
-> compiler for parellism: AutoFSDP??

----------------------------------
Part 2: Authoring vs Optimizations
----------------------------------

-----------------------------------------------------
Part 3: Tensor Compilers — Software 2.0's Dragon Book
-----------------------------------------------------


curve: pytorch -> triton -> thunderkittens? -> cutlass -> cuda -> ptx -> sass
-> we don't have good tensor languages yet.
-> no one disagrees that ergonomics don't matter.
-> the question is what level of expression is best for end users.
mapping current problems to existing ones
appendix A c2r5 is a c to risc 5 compiler.
- beth got them all optimizations. 80/20: DCE, elimination... appendix available
- we'll do the same: 
- bitter lesson/scaling laws: we found AR/NTP via attention operator from transformer
- $Bs. datacenter training runs.

- all the fundamental assumptions in the design of existing computing infrastructure
  need to be re-evaluated
- return of the mel: (kernel authoring, avoiding optimizations)
- instead of calling them l33t we call them cracked. same thing.
- https://users.cs.utah.edu/~elb/folklore/mel.html
-> tensor languages are not there yet

what's old is new.

1.0 problem to software 2.0
-> golden age ==> return of the Mel programmer because abstractions are in flux.
-> you can’t vibe code your way through classic CS problems.