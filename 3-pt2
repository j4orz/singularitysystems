            mm            
            MM            
`7MMpdMAo.mmMMmm  pd*"*b. 
  MM   `Wb  MM   (O)   j8 
  MM    M8  MM       ,;j9 
  MM   ,AP  MM    ,-='    
  MMbmmd'   `MbmoAmmmmmmm 
  MM                      
.JMML.

Singularity Systems: Zero to Hero

--------------------------------------------------------------------------------
Contents:
  0. au197
  1. dfdx(nd)
  2. brrr
  3. pt2 <-- HERE
    Part 0: dlpl — deep learning f̶r̶a̶m̶e̶w̶o̶r̶k̶s̶ programming languages
    Part 1: frontend — graph capture
    Part 2: backend — optimization & generation
  4. cloud
Appendix:
  A. c2r5
  B. trs
  C. licurs
--------------------------------------------------------------------------------

-------------------------------------------------------------
Part 0: DLPL — Deep Learning F̶r̶a̶m̶e̶w̶o̶r̶k̶s̶ Programming Languages
-------------------------------------------------------------
For the past decade ever since deep learning has entered the zeitgeist of the
academic community[0] and public consciousness[1], frameworks have been
researching for the right programming model to express deep neural networks. In
terms of user and execution models, the industry started with TF1's graph-graph
approach, and settled on PT1's eager-eager approach. Given that

  1. host-device execution is asynchronous and
  2. most of wall clock time on training and inference was spent on matmul FLOPS

as long as Python could schedule workloads so as to saturate the GPU's work queue,
then PT1's eager-eager approach was a zero cost abstraction. That is, there is no
other optimizations to make given that NVIDIA's best of the best were hand tuning
their vendor libraries such as cuBLAS and cuDNN.

However, the attention operator which unlocked the success of large autoregressive
models have precipated a change in the underlying hardware such as tensor cores,
tensor memory access, and more (TODO). While the good news is that pure tensor
contractions become faster, the overall wall clock time is dominated by slower
operations. This is the primary motivation for PT2's[2] eager-graph approach.
With fusing, unrolling, TODO.... there is room left on the table for compiler
optimizations:

TODO: ascii graph for matmul FLOPS vs non matmul FLOPS


--------------------------------
Part 1: Frontend — Graph Capture
--------------------------------
  1a - foo: torch.jit.trace
  1b — bar: torch.fx.symbolic_trace
  1c — Dynamic Bytecode Transformation: TorchDynamo





References:
productivity-performance-pareto
- DLPL: https://math.mit.edu/~edelman/publications/on_machine_learning.pdf
- graph-graph: TF1/caffe/theano https://arxiv.org/abs/1605.08695, https://dl.acm.org/doi/abs/10.1145/2647868.2654889, https://arxiv.org/abs/1605.02688
- eager-eager: Chainer/PT1/TF2 https://arxiv.org/abs/1908.00213, https://arxiv.org/pdf/1912.01703, https://arxiv.org/abs/1903.01855
- eager-graph (compiler):
  - PT2 https://arxiv.org/abs/2002.03794, https://dl.acm.org/doi/pdf/10.1145/3620665.3640366, https://www.youtube.com/watch?v=WxYEoTLgdLo
  - TVM: https://arxiv.org/abs/1802.04799

frontend: graph capture
1. jit.trace
2. fx.trace? symbolic tracing
- https://mlsys.org/Conferences/doc/2018/146.pdf
3. dynamic bytecode transforms
- https://arxiv.org/abs/2112.08429
- https://pytorch.org/docs/stable/torch.compiler_dynamo_overview.html
- https://pytorch.org/docs/stable/torch.compiler_dynamo_deepdive.html
- https://www.youtube.com/watch?v=egZB5Uxki0I
- https://www.youtube.com/watch?v=5FNHwPIyHr8

backend: codegen