                        __,                     
 ,6"Yb.`7MM  `7MM      `7MM  .d*"*bg. M******A' 
8)   MM  MM    MM        MM 6MP    Mb Y     A'  
 ,pm9MM  MM    MM        MM YMb    MM      A'   
8M   MM  MM    MM        MM  `MbmmdM9     A'    
`Moo9^Yo.`Mbod"YML.    .JMML.     .M'    A'     
                                .d9     A'      
                              m"'      A'       

Singularity Systems: Zero to Hero
--------------------------------------------------------------------------------
Syllabus:
  0. au197 <-- HERE
    Part 1: Golden Age — Software 2.0 Infrastructure Buildout
    Part 2: Tensor Compilers — Software 2.0 Dragon Book
  1. dfdx(nd)
  2. brrr
  3. pt2
  4. cloud
Appendix:
  A. c2r5
--------------------------------------------------------------------------------

---------------------------------------------------------
Part 1: Golden Age — Software 2.0 Infrastructure Buildout
---------------------------------------------------------
Welcome to the second golden age of computing. The first one started in the 1940s
when humanity discovered how to mechanize reprogrammable instructions (algorithms)
via deterministic languages encoded with 0s and 1s. The second one started in the
2020s when we discovered how to compress sequences via stochastic distributions
(models) with automatic differentiation and gradient descent.

In transportation technology is when humanity wants to go to the moon, engineers
reassess the design of vehicles from first principles. Given the workload of
space travel, it's economically feasiable to start from fundamentals and design
a rocket rather than a car or bus.

The same tension applies to intelligence technology — ever since the ChatGPT
and DeepSeek moment, engineers are reassessing the design of computers from
first principles. All of the original assumptions that were made last century
need to be reevaluated if the significant mode of computation is evaluating
matrix multiplications for stochastic distributions rather than fetching data
for discrete algorithms.

As a result, compiler buffs and chip architects report that the field is currently
undergoing a Golden Age:
1. A New Golden Age for Computer Architecture[0] — Hennessy and Patterson
2. The Golden Age of Compiler Design[1] — Chris Lattner

Many toy autograds exist — the software 2.0[2] version of calculator interpreters
capable of evaluating arithmetic. These are excellent in providing intuition for
backpropagation (calculus on a computational graph), a very important abstraction
to understand considering how leaky it is (think gradient initialization and
normalization). However, there are zero resources that dive deeper into cutting
your teeth on the advanced capabilities of PyTorch such as torch.compile and
torch distributed. Linux has xv6[3], clang has chibicc[4], and llvm has qbe[5],
but pytorch is missing it's teaching compiler. This is the gap that the course
fills.

The field of artificial intelligence is no stranger to pedagogical gaps.
Researchers talk about "research debt"[6] with respect to model development, but
what they forget to talk about is that the debt also applies to the underlying
infrastructure that powers the training and inference of deep neural networks.
This is largely in part because it's only been a decade since deep learning
has entered the zeitgeist of the research community[7] and public consciousness[8].

To give you a better idea of how nascent system ML as a field:
  1. the leading conferences for AI (NeurIPS) and Hardware (HOTCHIPS) have both
     been running for ~30 years[9][10]. Meanwhile, the conference for the
     intersection of the two (mlsys) has only been running for 6[11].
  2. the lingua franca deep learning framework was released[12] less than a
     decade, and the 2.0 release[13] was released only 2 years ago

Welcome to the golden age of Systems ML!

References
----------
[0] https://www.youtube.com/watch?v=3LVeEjsn8Ts
[1] https://www.youtube.com/watch?v=4HgShra-KnY
[2] https://karpathy.medium.com/software-2-0-a64152b37c35
[3]: https://github.com/mit-pdos/xv6-public
[4]: https://github.com/rui314/chibicc
[5]: https://c9x.me/compile/
[6]: https://distill.pub/2017/research-debt/
[7]: https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
[8]: https://sci-hub.se/https://doi.org/10.1038/nature14539
[9]: https://papers.nips.cc/
[10]: https://hotchips.org/archives/
[11]: https://proceedings.mlsys.org/
[12]: https://soumith.ch/blog/2023-12-17-pytorch-design-origins.md.html
[13]: https://pytorch.org/assets/pytorch2-2.pdf