                        __,                     
 ,6"Yb.`7MM  `7MM      `7MM  .d*"*bg. M******A' 
8)   MM  MM    MM        MM 6MP    Mb Y     A'  
 ,pm9MM  MM    MM        MM YMb    MM      A'   
8M   MM  MM    MM        MM  `MbmmdM9     A'    
`Moo9^Yo.`Mbod"YML.    .JMML.     .M'    A'     
                                .d9     A'      
                              m"'      A'       

Singularity Systems: Zero to Hero
--------------------------------------------------------------------------------
Syllabus:
  0. au197 <-- HERE
    Part 1: Golden Age — Software 2.0 Infrastructure Buildout
    Part 2: Tensor Compilers — Software 2.0 Dragon Book
  1. dfdx(nd)
  2. brrr
  3. pt2
  4. cloud
Appendix:
  A. c2r5
--------------------------------------------------------------------------------

---------------------------------------------------------
Part 1: Golden Age — Software 2.0 Infrastructure Buildout
---------------------------------------------------------
Welcome to the second golden age of computing. The first one started in the 1940s
when humanity discovered how to mechanize reprogrammable instructions (algorithms)
via deterministic languages encoded with 0s and 1s. The second one started in the
2020s when we discovered how to compress sequences via stochastic distributions
(models) with automatic differentiation and gradient descent.

In transportation technology is when humanity wants to go to the moon, engineers
reassess the design of vehicles from first principles. Given the workload of
space travel, it's economically feasiable to start from fundamentals and design
a rocket rather than a car or bus.

The same tension applies to intelligence technology — ever since the ChatGPT
and DeepSeek moment, engineers are reassessing the design of computers from
first principles. All of the original assumptions that were made last century
need to be reevaluated if the significant mode of computation is evaluating
matrix multiplications for stochastic distributions rather than fetching data
for discrete algorithms.

Building the infrastructure that underlies powerful neural networks such as
OpenAI's ChatGPT and DeepSeek's R1 is an *open* research problem: the deep
learning framework that is the lingua franca for most researchers was released[1]
less than a decade ago during my senior year of highschool, not to mention the
2.0 release[2] with torch.compile functionality being released in 2023!

As a result, compiler buffs and chip architects report that the field is currently
undergoing a Golden Age:

1. A New Golden Age for Computer Architecture — Hennessy and Patterson 2017
   -> https://www.youtube.com/watch?v=3LVeEjsn8Ts
2. The Golden Age of Compiler Design — Chris Lattner 2021
   -> https://www.youtube.com/watch?v=4HgShra-KnY

because of the golden age..everyone is rushing for gold. but there's a
pedagogical gap researchers label as "research debt".

Many toy autograds exist — the software 2.0 version of calculator interpreters
capable of evaluating arithmetic. These are excellent in providing intuition for
backpropagation (calculus on a computational graph), a very important abstraction
to understand considering how leaky it is (think gradient initialization and
normalization).

However, there are *zero* resources that dive deeper into cutting your teeth on
the advanced capabilities of PyTorch such as torch.compile and D(istributed)Tensor.
Linux has xv6[6], clang has chibicc[7], and llvm has qbe[8], but pytorch is
missing it's teaching tensor compiler!

-----------------------------------------------------
Part 2: Tensor Compilers — Software 2.0's Dragon Book
-----------------------------------------------------


curve: pytorch -> triton -> thunderkittens? -> cutlass -> cuda -> ptx -> sass
-> we don't have good tensor languages yet.
-> no one disagrees that ergonomics don't matter.
-> the question is what level of expression is best for end users.
mapping current problems to existing ones
appendix A c2r5 is a c to risc 5 compiler.
- beth got them all optimizations. 80/20: DCE, elimination... appendix available
- we'll do the same: 
- bitter lesson/scaling laws: we found AR/NTP via attention operator from transformer
- $Bs. datacenter training runs.

- all the fundamental assumptions in the design of existing computing infrastructure
  need to be re-evaluated
- return of the mel: (kernel authoring, avoiding optimizations)
- instead of calling them l33t we call them cracked. same thing.
- https://users.cs.utah.edu/~elb/folklore/mel.html
-> tensor languages are not there yet

what's old is new.

1.0 problem to software 2.0
-> golden age ==> return of the Mel programmer because abstractions are in flux.
-> you can’t vibe code your way through classic CS problems.




- software2.0: https://karpathy.medium.com/software-2-0-a64152b37c35
[0]: https://distill.pub/2017/research-debt/
[1]: https://soumith.ch/blog/2023-12-17-pytorch-design-origins.md.html
[2]: https://pytorch.org/assets/pytorch2-2.pdf
[3]: https://papers.nips.cc/
[4]: https://hotchips.org/archives/
[5]: https://proceedings.mlsys.org/
[6]: https://github.com/mit-pdos/xv6-public
[7]: https://github.com/rui314/chibicc
[8]: https://c9x.me/compile/
[9]: https://docs.google.com/spreadsheets/d/1WKHbT-7KOgjEawq5h5Ic1qUWzpfAzuD_J06N1JwOCGs