                        __,                     
 ,6"Yb.`7MM  `7MM      `7MM  .d*"*bg. M******A' 
8)   MM  MM    MM        MM 6MP    Mb Y     A'  
 ,pm9MM  MM    MM        MM YMb    MM      A'   
8M   MM  MM    MM        MM  `MbmmdM9     A'    
`Moo9^Yo.`Mbod"YML.    .JMML.     .M'    A'     
                                .d9     A'      
                              m"'      A'       

Singularity Systems: Zero to Hero
--------------------------------------------------------------------------------
Syllabus:
  0. au197 <-- HERE
  1. dfdx(nd)
  2. brrr
  3. pt2
  4. cloud
Appendix:
  A. c2r5
--------------------------------------------------------------------------------

---------------------------------------------------------
Part 1: Golden Age — Software 2.0 Infrastructure Buildout
---------------------------------------------------------
Welcome to the second golden age of computing. The first one started in the '40s
when humanity discovered how to mechanize reprogrammable instructions (algorithms)
via deterministic languages encoded with 1s and 0s. The second one started at the
beginning of this decade when we discovered how to capture natural language via
stochastic distributions (models) with automatic differentiation and gradient descent.

- bitter lesson/scaling laws: we found AR/NTP via attention operator from transformer
- $Bs. datacenter training runs.

- all the fundamental assumptions in the design of existing computing infrastructure
  need to be re-evaluated
- return of the mel: (kernel authoring, avoiding optimizations)
- instead of calling them l33t we call them cracked. same thing.
- https://users.cs.utah.edu/~elb/folklore/mel.html
-> tensor languages are not there yet

what's old is new.

golden age: massive infrastructure buildout

--------------------------------------------
Part 2: Tensor Compilers
--------------------------------------------

1.0 problem to software 2.0
-> golden age ==> return of the Mel programmer because abstractions are in flux.
-> you can’t vibe code your way through classic CS problems.


curve: pytorch -> triton -> thunderkittens? -> cutlass -> cuda -> ptx -> sass


-> we don't have good tensor languages yet.
-> no one disagrees that ergonomics don't matter.
-> the question is what level of expression is best for end users.
mapping current problems to existing ones
appendix A c2r5 is a c to risc 5 compiler.


TODO: python is "growable" language (xkcd comic)
training machine learning models is faster in pytorch than c++/rust.
pytorch 2.0 is pure python, CUTLASS 4.0 is pure python
-> community supported ports? i'll start with rust.


-----------------------------------------------------------------------------------
Part 3: Singularity Systems — Zero to Hero (The Hacker's Guide to Tensor Compilers)
-----------------------------------------------------------------------------------

Researchers in artificial intelligence talk about research debt[0] with respect
to model development. Well, we also see this pedagogical gap for the underlying
infrastructure that powers modern-day models. That is, building deep learning
compilers (and chips) is currently an open research problem: todays leading deep
learning framework was released[1] less than a decade ago during my senior year
of highschool, not to mention the 2.0 release[2] with torch.compile functionality
being released in 2023.

At the time of writing, there are 37 symposiums in the list of NeurIPS
proceedings[3], 36 symposiums in the list of Hot Chips proceedings[4], but only
6 symposiums in the list of MLSys proceedings[5]. Many toy autograds exist
which are the software 2.0 equivalent of calculator interpreters capable of
evaluating arithmetic. These are excellent in providing intuition for
backpropagation (calculus on a computational graph), a very important abstraction
to understand considering how leaky it is (think gradient initialization and
normalization). However, there are absolutely zero resources that dive deeper
into cutting your teeth on the advanced capabilities of PyTorch such as
torch.compile and D(istributed)Tensor. If linux has xv6[6], clang has chibicc[7],
and llvm has qbe[8], then pytorch is missing its equivalent. The next best thing
is to peek behind the curtains at the actual PyTorch source which is close to
impossible with its hundreds of thousands of lines of code.

The second section covers the static compilation of neural networks like
PyTorch 2.0.

There's also a brief appendix on how traditional compiler techniques and
infrastructure transfer over to tensor compilers by building a compiler that maps
C89 to RV32I. TODO: mapping current problems in ml systems to traditional cs problems.


- software2.0: https://karpathy.medium.com/software-2-0-a64152b37c35


[0]: https://distill.pub/2017/research-debt/
[1]: https://soumith.ch/blog/2023-12-17-pytorch-design-origins.md.html
[2]: https://pytorch.org/assets/pytorch2-2.pdf
[3]: https://papers.nips.cc/
[4]: https://hotchips.org/archives/
[5]: https://proceedings.mlsys.org/
[6]: https://github.com/mit-pdos/xv6-public
[7]: https://github.com/rui314/chibicc
[8]: https://c9x.me/compile/
[9]: https://docs.google.com/spreadsheets/d/1WKHbT-7KOgjEawq5h5Ic1qUWzpfAzuD_J06N1JwOCGs