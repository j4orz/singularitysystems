                        __,                     
 ,6"Yb.`7MM  `7MM      `7MM  .d*"*bg. M******A' 
8)   MM  MM    MM        MM 6MP    Mb Y     A'  
 ,pm9MM  MM    MM        MM YMb    MM      A'   
8M   MM  MM    MM        MM  `MbmmdM9     A'    
`Moo9^Yo.`Mbod"YML.    .JMML.     .M'    A'     
                                .d9     A'      
                              m"'      A'       

Singularity Systems: Zero to Hero
--------------------------------------------------------------------------------
Syllabus:
  0. au197 <-- HERE
  1. dfdx(nd)
  2. brrr
  3. pt2
  4. cloud
Appendix:
  A. c2r5
  B. shapes
--------------------------------------------------------------------------------

TODO: python is "growable" language (xkcd comic)
training machine learning models is faster in pytorch than c++/rust.
pytorch 2.0 is pure python, CUTLASS 4.0 is pure python
-> community supported ports? i'll start with rust.

golden age: massive infrastructure buildout
-> return of the Mel programmer because abstractions are in flux.
-> golden age: compiler engineer. Database engineer. Crypto engineer. Reduce your software 1.0 problem to software 2.0
-> you canâ€™t vibe code you way through classic CS problems.

















Researchers in artificial intelligence talk about research debt[0] with respect
to model development. The same applies to the infrastructure that underlies these
deep neural networks. Building deep learning compilers (and chips) is currently
an open research problem: todays leading deep learning framework was released[1]
less than a decade ago during my senior year of highschool, not to mention the
2.0 release[2] with torch.compile functionality being released in 2023.

At the time of writing, there are 37 symposiums in the list of NeurIPS
proceedings[3], 36 symposiums in the list of Hot Chips proceedings[4], but only
6 symposiums in the list of MLSys proceedings[5]. Many toy autograds exist
which are the software 2.0 equivalent of calculator interpreters capable of
evaluating arithmetic. These are excellent in providing intuition for
backpropagation (calculus on a computational graph), a very important abstraction
to understand considering how leaky it is (think gradient initialization and
normalization). However, there are absolutely zero resources that dive deeper
into cutting your teeth on the advanced capabilities of PyTorch such as
torch.compile and D(istributed)Tensor. If linux has xv6[6], clang has chibicc[7],
and llvm has qbe[8], then pytorch is missing its equivalent. The next best thing
is to peek behind the curtains at the actual PyTorch source which is close to
impossible with its hundreds of thousands of lines of code.

The second section covers the static compilation of neural networks like
PyTorch 2.0.

There's also a brief appendix on how traditional compiler techniques and
infrastructure transfer over to tensor compilers by building a compiler that maps
C89 to RV32I. TODO: mapping current problems in ml systems to traditional cs problems.


[0]: https://distill.pub/2017/research-debt/
[1]: https://soumith.ch/blog/2023-12-17-pytorch-design-origins.md.html
[2]: https://pytorch.org/assets/pytorch2-2.pdf
[3]: https://papers.nips.cc/
[4]: https://hotchips.org/archives/
[5]: https://proceedings.mlsys.org/
[6]: https://github.com/mit-pdos/xv6-public
[7]: https://github.com/rui314/chibicc
[8]: https://c9x.me/compile/
[9]: https://docs.google.com/spreadsheets/d/1WKHbT-7KOgjEawq5h5Ic1qUWzpfAzuD_J06N1JwOCGs