                  ,,              ,,      ,...    ,,             
                `7MM            `7MM    .d' ""  `7MM             
                  MM              MM    dM`       MM             
`7MMpMMMb.   ,M""bMM         ,M""bMM   mMMmm ,M""bMM  `7M'   `MF'
  MM    MM ,AP    MM       ,AP    MM    MM ,AP    MM    `VA ,V'  
  MM    MM 8MI    MM mmmmm 8MI    MM    MM 8MI    MM      XMX    
  MM    MM `Mb    MM       `Mb    MM    MM `Mb    MM    ,V' VA.  
.JMML  JMML.`Wbmd"MML.      `Wbmd"MML..JMML.`Wbmd"MML..AM.   .MA.
                                                                 
Singularity Systems: Zero to Hero
--------------------------------------------------------------------------------
Syllabus:
  1. nd-dfdx <-- HERE
  2. brrr
  3. pt2
  4. cloud
Appendix:
  A. c2r5
  B. shapes
--------------------------------------------------------------------------------

Welcome to the beginning of a whirlwind tour on deep learning frameworks!
Throughout this journey you will build an interpreter and compiler for neural
networks line by line, from scratch. By the end of the first chapter, you will
have a working implementation of an interpreter that provides an abstraction for
the multidimensional array (like numpy's `ndarray` and pytorch's `tensor`) with
autodifferentiation capabilities to make it *learn*. This interpreter is capable
of training and inferencing the simple FFN (also refferred to as MLP) following
(Bengio et al. 2003), covered in Neural Networks: Zero to Hero part 2. We will
keep the implementation simple using the host language's CPU support — in the
third chapter we will accelerate our computations using SIMD capability by the
CPU and GPU.

------------------------------
Part 1: multidimensional array
------------------------------
Let's start with the multidimensional array and follow pytorch's naming convention
by calling it tensor (and overloading its usage in physics which has invariants
that do not hold in statistical learning). Consider the following product type
for tensor:

prod Tensor {
    ndim: int
    shape: int[]
    stride: int[]
    storage: float[]
}

where ndim is an integer, shape as well as stride are lists of integers, and
finally, storage is list of floats. Defining the tensor this way (rather than
naively using nested arrays) makes the aliasing of underlying data buffers quite
natural, since  physical storage can be logically interpreted using shape and
stride. This is desirable in deep learning since many tensor operations only
involve modifying the shape and stride, thereby updating the "view" of the tensor.
Consider example 1 and 2 which illustrate that operations such as reshape and
transpose fundamentally do not add or remove elements from the tensor — they only
move data around. TODO: not really data. but logical interpretation.

Example 1:

01 02 03   reshape      01 02
04 05 06     ==>        03 04

07 08 09                05 06
10 11 12                07 08

                        09 10
                        11 12

Example 2:

1 2 3    transpose    1 4 7
4 5 6      ==>        2 5 8
7 8 9                 3 6 9

Ops
- uops: neg, exp, log, tanh
- binops: add, sub, mul, div
- reduceops: sum, argmax, softmax, max, min, mean, var, std, logsumexp
- processingops: matmul, conv2d, pool2d, maxpool2d, avgpool2d,
- nnops?: linear, sequential, layernorm, batchnorm, crossentropyloss, nllloss

TODO: why does numpy-C api only support dimension up to 6?

---------------------------
Part 2: autodifferentiation
---------------------------
We have built an abstraction for the multidimensional array placing our current
framework more or less on par with the capabilities of numpy. But in order to
*train* neural networks we need to add support for autodifferentiation, following
the footsteps of Wiltschko's torch-autograd (influencing PyTorch) and
Johnson/Maclaurin's HIPS/autograd (influencing JAX).

Derivative: approximation via local linearization
---------------------------------------------------------
The first place to start is to clarify and disambiguate the notion of a derivative
in order to generalize the denotation we use for higher dimensions to hold the
more precise semantics of approximating complex surfaces with a *local linearization*.
That is, we'll define the derivative as a *linear operator* (also known as the
Fréchet derivative).

Recall that for some f: ℝ -> ℝ the difference of a function at a point x and
x+δx is f(x+δx)-f(x) = δf which can be approximated by f'(x)δx plus some error
terms. That is,

f(x+δx)-f(x) = δf
             ≈ f'(x)δx + o(δx)

and if we take the difference to be an infinitesimal then it becomes a differential

f(x+dx)-f(x) = df
             = f'(x)dx

and more generally we define the derivative *as* the linear operator L on vector
space V which you apply to a change in input in order to obtain the change in
output. That is, Δout = (lin.op)Δin. which generalizes for higher dimensions.
While defining f'x ≜ df/dx is illegal when f: ℝ -> ℝ, it's not clear what df/dx
means in higher dimensions. That is, /: ℝ^d -> ℝ^d is usually not defined.

Consider df = f'(x)dx when f: ℝ^d -> ℝ. Then, given that dx ∈ ℝ and df ∈ ℝ^d,
the linear operator f'(x) *must* be the dot product with some vector, (or
multiplication with the vector transposed). Let's call this vector the "gradient"
of f and as you can now probably tell, this is *why* the gradient is defined the
way it is. Moving on, if we consider df = f'(x)dx where f: ℝ^n -> ℝ^m, then
given that dx ∈ ℝ^n and df ∈ ℝ^m, the linear operator f'(x) *must* be expressable
as a matrix multiplication with some matrix A ∈ ℝ^(mxn) since any
linear transformation is either a scaling, shearing, rotation or reflection.
Let's call this matrix the "jacobian".

Autodifferentiation: calculus on computational graph
----------------------------------------------------
TODO: reverse mode (backprop) and forward mode (DP)

-----------------------
Citation and References
-----------------------
Zhang, Jeff. (Mar 2025). Matrix Calculus for Deep Learning Frameworks.
Singularity Systems. https://github.com/j4orz/singularitysystems/blob/master/1-nd-dfdx.
```
@article{weng2024rewardhack,
  title   = "Matrix Calculus for Deep Learning Frameworks",
  author  = "Zhang, Jeff",
  journal = "https://github.com/j4orz/singularitysystems",
  year    = "2025",
  month   = "Mar",
  url     = "https://github.com/j4orz/singularitysystems/blob/master/1-nd-dfdx"
}
```

References:
[0] https://numpy.org/doc/stable/dev/internals.html
[1] https://numpy.org/doc/stable/dev/internals.code-explanations.html
[2] https://numpy.org/doc/stable/reference/arrays.ndarray.html#internal-memory-layout-of-an-ndarray
[3] http://blog.ezyang.com/2019/05/pytorch-internals/
[4] https://arxiv.org/pdf/1912.01703
[5] https://docs.jax.dev/en/latest/autodidax.html