<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Singularity Systems: Zero to Hero</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Singularity Systems: Zero to Hero</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="dedication"><a class="header" href="#dedication">Dedication</a></h1>
<p>This "textbook" is dedicated to my father Thomas, brother Leo, and partner Anna
who have helped me through my original plans for graduate school needing to be
redirected towards helping my sister's diagnosis with schizophrenia. I have learned
more about what life is truly about off the keyboard than I ever could have
hacking on a terminal. Although I was not able to pursue my original goals
(at this time), I hope Singularity Systems can provide some use to the future
generation of programmers following the ways of old school hacker culture.</p>
<p>- <a href="https://github.com/j4orz/">j4orz</a></p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="./flammarion.webp" alt="" /></p>
<h1 id="singularity-systems-zero-to-hero"><a class="header" href="#singularity-systems-zero-to-hero">Singularity Systems: Zero to Hero</a></h1>
<p><em><strong>The Hacker's Guide to Tensor Compilers</strong></em></p>
<h2 id="course-description"><a class="header" href="#course-description">Course Description</a></h2>
<p>Singularity Systems: Zero to Hero follows up from
<a href="https://karpathy.ai/zero-to-hero.html">Neural Networks: Zero to Hero</a>. We convert</p>
<ul>
<li><a href="https://github.com/karpathy/micrograd"><strong>micrograd</strong></a>: toy backpropagation engine <em>into...</em></li>
<li><a href="https://github.com/j4orz/picograd"><strong>picograd</strong></a>: modern deep learning framework</li>
</ul>
<p>While micrograd is good for research scientists to further understand the leaky
abstraction of backpropagation (think activation gradients and normalization),
picograd is intended for systems programmers and performance engineers looking
to further understand the compilers and chips that power machine learning.</p>
<h3 id="prereqs"><a class="header" href="#prereqs">Prereqs</a></h3>
<ul>
<li>solid deep learning (sd &amp;&amp; llama)</li>
<li>solid systems programming (C || C++ || Rust* || Python**)
<ul>
<li>*rust is currently the only supported language</li>
<li>**python is ok (following pure-python systems like PyTorch 2.0 &amp; CUTLASS 4.0)</li>
</ul>
</li>
</ul>
<h3 id="syllabus"><a class="header" href="#syllabus">Syllabus</a></h3>
<ol>
<li><strong>au197</strong>: introduces the golden age of systems ml.</li>
<li><strong>dfdx(nd)</strong>: implements an interpreter for neural networks (pytorch1).</li>
<li><strong>pt2</strong>: implements a compiler for neural networks (pytorch2).</li>
<li><strong>tiles</strong>: implements a compiler for a tiling language (triton).</li>
<li><strong>turing</strong>: implements a compiler that maps C to RISCV.</li>
</ol>
<h3 id="citation"><a class="header" href="#citation">Citation</a></h3>
<pre><code>@article{j4orz2025singsys,
  author  = "j4orz",
  title   = "Singularity Systems: Zero to Hero",
  year    = "2025",
  url     = "https://github.com/j4orz/singularitysystems"
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-1-au-197"><a class="header" href="#chapter-1-au-197">Chapter 1: au-197</a></h1>
<p><strong>This chapter introduces the golden age of systems ml</strong></p>
<h2 id="golden-age--software-20-infrastructure-buildout"><a class="header" href="#golden-age--software-20-infrastructure-buildout">Golden Age — Software 2.0 Infrastructure Buildout</a></h2>
<p>Welcome to the second golden age of computing. The first one took place in the
1940s when we discovered how to program sequences of instructions on
latency-oriented machines with scalar processing. We collectively refer to this
period as the <em>information revolution</em>. The second one is taking place today in
the 2020s where we are discovering how to compress sequences with stochastic
distributions on throughput-oriented machines with vector processing. People are
describing this period as the beginning of the <em>intelligence revolution</em>.</p>
<p>What we see in both periods is the occurence of positive-sum-game-generating
socioeconomic feedback loops dubbed "scaling laws" between software and hardware.
That is, the more demand generated from applications, the more supply from
infrastructure, which unlocks better applications, which generates more demand,
which increases the supply, and so on. The information revolution gave rise
to Moore's Law, an exponential trend between transistor density and TODO: X,
while the intelligence revolution today gives rise to Scaling Law's, another
exponential between compression and parameter count.</p>
<p>This textbook is all about the latter. More specifically we focus our attention
on the <strong>compilers and chips</strong> that are powering the massive AI infrastructure
buildout that's happening today. What's exciting from the perspective of a
compiler engineer or chip architect is that all of the assumptions held in the
design of our computers need to be reassessed and reevaluated. This is because
the "soul of the machine" is constructed by the dance that happens between the
compiler and chip — <em>together</em> through software-hardware codesign they bridge
the semantic gap between humans and electrons.</p>
<p>The original assumption held in the computers designed from the information
revolution was <a href="">Dennard Scaling</a> (also referred to as MOSFET Scaling) which
states that as the size of transistors decreases, the power density stays
constant.</p>
<p>TODO: von neumann prog/exec model.
The von neumann</p>
<p>What's interesting to systems programmers and performance engineers is that the
field is witnessing the rhyming of history. The feedback loop between the
information revolution's software applications and sequential hardware
(OOO, pipelining, TODO) creates this sociocultural phenomena dubbed
"Moore's Law"[X]. The same loop is happening with the intelligence revolution's
models and parallel hardware (tensor cores, TMA, TODO) which is now giving rise
to the "Scaling Laws"[X]. All of the original assumptions held in the design of
the compilers and chips that make up the soul of the machine are being reassessed
if the dominant workload is evaluating matrix multiplications for stochastic
distributions instead of fetching data for discrete algorithms.</p>
<p>In transportation technology is when humanity wants to go to the moon, engineers
reassess the design of vehicles from first principles. Given the workload of
space travel, it's economically feasiable to start from fundamentals and design
a rocket rather than a car or bus. As a result, the engineers behind the massive
infrastructure buildout are dubbing this period of
history as a new golden age:</p>
<ol>
<li>A New Golden Age for Computer Architecture[0] — Hennessy and Patterson</li>
<li>The Golden Age of Compiler Design[1] — Chris Lattner</li>
</ol>
<p>Many toy autograds exist — the software 2.0[2] version of calculator interpreters
capable of evaluating arithmetic. These are excellent in providing intuition for
backpropagation (calculus on a computational graph), a very important abstraction
to understand considering how leaky it is (think gradient initialization and
normalization). However, there are zero resources that dive deeper into cutting
your teeth on the advanced capabilities of PyTorch such as torch.compile and
torch distributed. Linux has xv6[3], clang has chibicc[4], and llvm has qbe[5],
but pytorch is missing it's teaching compiler. This is the gap that the course
fills.</p>
<p>The field of artificial intelligence is no stranger to pedagogical gaps.
Researchers talk about "research debt"[6] with respect to model development, but
what they forget to talk about is that the debt also applies to the underlying
infrastructure that powers the training and inference of deep neural networks.
This is largely in part because it's only been a decade since deep learning
has entered the zeitgeist of academia[7] and the public[8].</p>
<p>To give you a better idea of how nascent system ML as a field:</p>
<ol>
<li>the leading conferences for AI (NeurIPS) and Hardware (HOTCHIPS) have both
been running for ~30 years[9][10]. Meanwhile, the conference for the
intersection of the two (mlsys) has only been running for 6[11].</li>
<li>the lingua franca deep learning framework was released[12] less than a
decade, and the 2.0 release[13] was released only 2 years ago</li>
</ol>
<p>Welcome to the golden age of Systems ML!</p>
<h2 id="references"><a class="header" href="#references">References</a></h2>
<ol start="0">
<li><a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a></li>
<li><a href="https://www.nature.com/articles/nature14539">https://www.nature.com/articles/nature14539</a></li>
<li><a href="https://www.youtube.com/watch?v=3LVeEjsn8Ts">https://www.youtube.com/watch?v=3LVeEjsn8Ts</a></li>
<li><a href="https://www.youtube.com/watch?v=4HgShra-KnY">https://www.youtube.com/watch?v=4HgShra-KnY</a></li>
<li><a href="https://karpathy.medium.com/software-2-0-a64152b37c35">https://karpathy.medium.com/software-2-0-a64152b37c35</a></li>
<li><a href="https://github.com/mit-pdos/xv6-public">https://github.com/mit-pdos/xv6-public</a></li>
<li><a href="https://github.com/rui314/chibicc">https://github.com/rui314/chibicc</a></li>
<li><a href="https://c9x.me/compile/">https://c9x.me/compile/</a></li>
<li><a href="https://distill.pub/2017/research-debt/">https://distill.pub/2017/research-debt/</a></li>
<li><a href="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a></li>
<li><a href="https://www.nature.com/articles/nature14539">https://www.nature.com/articles/nature14539</a></li>
<li><a href="https://papers.nips.cc/">https://papers.nips.cc/</a></li>
<li><a href="https://hotchips.org/archives/">https://hotchips.org/archives/</a></li>
<li><a href="https://proceedings.mlsys.org/">https://proceedings.mlsys.org/</a></li>
<li><a href="https://soumith.ch/blog/2023-12-17-pytorch-design-origins.md.html">https://soumith.ch/blog/2023-12-17-pytorch-design-origins.md.html</a></li>
<li><a href="https://pytorch.org/assets/pytorch2-2.pdf">https://pytorch.org/assets/pytorch2-2.pdf</a></li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-2-dfdxnd"><a class="header" href="#chapter-2-dfdxnd">Chapter 2: dfdx(nd)</a></h1>
<p><strong>This chapter implements an interpreter for neural networks (pytorch1 "eager" mode).</strong>
As stated in the syllabus, solid deep learning and systems programming are
prerequisites. Section 1 on correctness assumes mathematical maturity with
statistics and matrix calculus. Section 2 assumes familiarity with out of order
and superscalar processors. By the end of this chapter, you will have an
accelerated implementation of the multidimensional array with autodifferentiation
capability capable of interpreting llama and sd.</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<p><a href=""><strong>Section 1: Correctness</strong></a></p>
<ol start="0">
<li><a href="">non-linear parametric models: <code>nn.Linear()</code>, <code>nn.ReLU()</code></a></li>
<li><a href="">multidimensional array: <code>model.forward()</code></a></li>
<li><a href="">gradient descent: <code>loss.backward()</code>, <code>opt.step()</code></a></li>
<li><a href="">attention: a(q,k,v)</a></li>
<li><a href="">thought:</a></li>
</ol>
<p><a href=""><strong>Section 2: Speed</strong></a></p>
<ol start="5">
<li><a href="">CPU (SIMD on SIMD): <code>AVX512</code>, <code>AMX</code>, <code>NEON</code></a></li>
<li><a href="">GPU (SIMT on SIMD):</a></li>
<li><a href="">TPU (systolic):</a></li>
</ol>
<h1 id="section-1-correctness"><a class="header" href="#section-1-correctness">Section 1: Correctness</a></h1>
<h2 id="part-0-non-linear-parametric-models-nnlinear-nnrelu"><a class="header" href="#part-0-non-linear-parametric-models-nnlinear-nnrelu">Part 0: non-linear parametric models: <code>nn.Linear()</code>, <code>nn.ReLU()</code></a></h2>
<p>Before jumping into the implementation of our deep learning framework's
multidimensional array with autodifferentiation capability, let's review the
mathematics of neural networks. We will incrementally construct a family of
functions from logistic regression, multiclass regression, feedforward
neural networks, attention and chain of thought variants, all for the
classification setting where Y⊆ℕ.</p>
<p>Recall that the logistic regression model for binary classification recovers the
bernouilli distribution:</p>
<pre><code>    P: ℝ^d -&gt; [0,1] by assuming
    P(Y=1|X=x;θ={w,b}) := σ(wᵀx) ==&gt; P(Y=0|X=x;θ) = 1 - σ(wᵀx)
==&gt; P(Y=y|X=x;θ) = ŷ^y (1-ŷ)^(1-y) [continuous form]
                 = σ(wᵀx)^y [1-σ(wᵀx)]^(1-y)

+---+             
| x1+---+         
| x2+--+|w_0      
|   |  ||         
|   |w_1|         
| . |  |+----&gt;-+. 
| . |  +--&gt;(wᵀ |σ)
| . |  +---&gt;`--+' 
|   |  |          
|   |  |w_d       
|   |  |          
| xd+--+          
+---+
Fig 1. logistic regression
</code></pre>
<p>where we can omit the bias by prepending column vector x with x_0 := 1, using
using w_0 as b. From now on we will implicitly assume that the hyperparameter
d is d_old+1 so there is an extra row for the bias terms. And since we are
training from a dataset D={(x_i, y_i)} (i=1..n), we can evaluate the function
P: ℝ^d -&gt; [0,1], P(x^i;θ={w,b}) := σ(wᵀx^(i)) for all n input-output pairs with
a single matrix vector multiplication:
ŷ = σ(Xw, dim=0)
where:
X ∈ ℝ^{nxd}
w ∈ ℝ^d</p>
<p>We can also extend this model to multi-class classification by generalizing
logistic regression to softmax regression by replacing σ(z) with softmax(z):
σ: ℝ -&gt; [0,1], σ(z) := 1/[1+exp(-z)]
softmax: ℝ^k -&gt; [0,1]^k, softmax(z_i) := exp(z_i) / Σ exp(z_j)
j=1..k</p>
<p>recalling that 1. ∫softmax(z) = 1 and 2. that z is referred to as the logits
since sigmoid and softmax map ℝ to log odds (ln[p/(1-p)]). Generalizing sigmoid
to softmax allows us to recover the multinomial distribution:</p>
<pre><code>    P: ℝ^d -&gt; [0,1]^k by assuming
    P(Y=i|X=x) = exp(w_iᵀx) / Σ exp(w_jᵀx)
                              j=1..k
==&gt; P(Y=y|X=x) = softmax(Wx), where W ∈ ℝ^{kxd}, x ∈ ℝ^d : x_0 = 1

+---++---------------++----+                          +------------------------+
| x1||               ||w1ᵀσ|                          |exp(w1ᵀx) / Σ exp(w_jᵀx)|
| x2||               ||w2ᵀσ|                          |exp(w2ᵀx) / Σ exp(w_jᵀx)|
|   ||               ||    |                          |                        |
|   ||               ||    |                          |                        |
| . ||               || .  |    +----------------+    | .                      |
| . ||       W       || .  |---&gt;|g(z):=softmax(z)|---&gt;| .                      |
| . ||               || .  |    +----------------+    | .                      |
|   ||               ||    |                          |                        |
|   ||               ||    |                          |                        |
|   ||               ||    |                          |                        |
| xd||               ||wdᵀσ|                          |exp(wdᵀx) / Σ exp(w_jᵀx)|
+---++---------------++----+                          +------------------------+
Fig 2. softmax regression
</code></pre>
<p>And to evaluate P: ℝ^d -&gt; [0,1]^k, P(x^i;θ) := softmax(Wx) for all n input-output
pairs with a single matrix multiplication:
ŷ = softmax(XW, dim=0)
where:
X ∈ ℝ^{nxd}
W ∈ ℝ^{dxk}</p>
<p>Finally, we can extend the multinomial logistic regression by introducing
intermediate stages of computation to project the representation of inputs
into a basis that is more tractable when mapping to a distribution:</p>
<pre><code>    P: ℝ^d -&gt; [0,1]^k
    P(x;θ={w,b}) := (softmax ◦ W_L ◦ (φ ◦ W_{L-1}) ◦ ... ◦ (φ ◦ W_1))(x)

and each pair of linearity W and non-linearity φ is a "hidden layer" so
    h^(1) := φ(W^1x+b_1)
    h^(2) := φ(W^2h^(1)+b_2)
    .
    .
    .
    h^(L) := φ(W^Lh^(L-1)+b_L)

                                                                               +------------------------+
                                                                               |exp(w1ᵀx) / Σ exp(w_jᵀx)|
                                                                               |exp(w2ᵀx) / Σ exp(w_jᵀx)|
                                                                               |                        |
                      +---++---------------++-------+                          |                        |
                      | a1||               ||φ(w1ᵀσ)|    +----------------+    | .                      |
                      | a2||               ||φ(w2ᵀσ)|---&gt;|g(z):=softmax(z)|---&gt;| .                      |
                      |   ||               ||       |    +----------------+    | .                      |
  +-------------------+---++----+          ||       |                          |                        |
 ++----------------------------+|          || .     |                          |                        |
++--++---------------++-------+||  WL      || .     |                          |                        |
| x1||               ||φ(w1ᵀσ)|||          || .     |                          |exp(wdᵀx) / Σ exp(w_jᵀx)|
| x2||               ||φ(w2ᵀσ)|||          ||       |                          +------------------------+
|   ||               ||       |||  *       ||       |
|   ||               ||       ||| *        ||       |
| . ||               || .     |||*         ||φ(wdᵀσ)|
| . ||       W1      || .     ||+----------++-------+
| . ||               || .     |||
|   ||               ||       |||
|   ||               ||       |||
|   ||               ||       |++
| xd++               ++φ(wdᵀσ)++
+---++---------------++-------+
Fig 3. neural network
</code></pre>
<p>Recall that neural networks are the family of functions that are non-linear
and parametric. The non-linearities that act as feature extractors differentiate
this class from the family of linear functions such as linear and logistic
regression, and the parametric weights and biases differentiate the class from
other non-linear functions such as gaussian processes and kernel methods. Favoring
the mathematical specification over the biological inspiration, neural networks,
reductively put, are a lot of logistic regressions (weighted sums with
non-linearities) stacked together.</p>
<pre><code class="language-python">"""
model: Neural Language Models (Bengio et al. 2003) URL: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf

Dimension key:
# windows
B: batch size
T: sequence length

# input/output
V: vocabulary size
E: embedding dimension (E != D in paper)
D: model dimension
"""
import picograd
B, T = 32, 3
V, E, D = 27, 10, 200

# *********************MODEL*********************
class Linear:
  def __init__(self, D_in, D_out, bias=True):
    self.W_DiDo = picograd.randn((D_in, D_out)) * (5/3)/D_in**0.5 # kaiming init (He et al. 2015)
    self.b_Do = picograd.zeros(D_out) if bias else None

  def __call__(self, X_Di):
    self.X_Do = X_Di @ self.W_DiDo
    if self.b_Do is not None:
        self.X_Do += self.b_Do
    self.out = self.X_Do
    return self.X_Do

  def parameters(self):
    return [self.W_DiDo] + ([] if self.b_Do is None else [self.b_Do])

class Tanh:
  def __call__(self, X_BD):
    self.X_BD = picograd.tanh(X_BD)
    # plt.hist(self.X_BD.view(-1).tolist(), 50); # distribution of weights
    # plt.imshow(self.X_BD.abs() &gt; 0.99, cmap='gray', interpolation='nearest') # vanishing gradients
    self.out = self.X_BD
    return self.X_BD
  
  def parameters(self):
    return []

model = [
  Linear(T * E, D, bias=False), Tanh(),
  Linear(D, D, bias=False), Tanh(),
  Linear(D, V, bias=False)
]

C_VE = picograd.randn((V,E))

# *********************INFERENCE LOOP*********************
for _ in range(20): # 20 samples
  output, context = [], [0] * T
  while True:
    X_1T = picograd.tensor([context]) # B=1 for inference, T=3, in [0..27] (set to 0 for init)
    X_1TE = C_VE[X_1T] # using 0..27 as indices into C_VE for each B=1 example of context length T
    print(X_1TE)
    X_1cTE = X_1TE.view(-1, T*E) # B=1, TE
    X = X_1cTE

    for h in model:
      X = h(X)

    y_hat = F.softmax(X, dim=1)

    # sample and autoregressively update context
    token = picograd.multinomial(y_hat, num_samples=1, replacement=True).item()#, generator=g).item()
    context = context[1:] + [token]
    output.append(decode[token])
    if token == 0:
        break
  print(''.join(output))
</code></pre>
<h2 id="part-1-multidimensional-array-modelforward"><a class="header" href="#part-1-multidimensional-array-modelforward">Part 1: multidimensional array: <code>model.forward()</code></a></h2>
<p>Let's start with the multidimensional array and follow pytorch's <code>tensor</code> naming
convention. Consider the following product type for tensor:</p>
<p>struct Tensor {
ndim: i32
shape: Vec<usize>
stride: Vec<usize>
storage: Vec<f32>
}</p>
<p>where ndim is an integer, shape as well as stride are lists of integers, and
finally, storage is list of floats. Defining the tensor this way (rather than
naively using nested arrays) makes the aliasing of underlying data buffers quite
natural, since  physical storage can be logically interpreted using shape and
stride. This is desirable in deep learning since many tensor operations only
involve modifying the shape and stride, thereby updating the "view" of the tensor.
Consider example 1 and 2 which illustrate that operations such as reshape and
transpose fundamentally do not add or remove elements from the tensor — they only
change the interpretation of data:</p>
<p>Example 1:
01 02 03   reshape      01 02
04 05 06     ==&gt;        03 04</p>
<p>07 08 09                05 06
10 11 12                07 08</p>
<pre><code>                    09 10
                    11 12
</code></pre>
<p>Example 2:
1 2 3    transpose    1 4 7
4 5 6      ==&gt;        2 5 8
7 8 9                 3 6 9</p>
<p>TODO:</p>
<ul>
<li>binops (zip), uops(map), reduceops(), viewops, matmul.</li>
</ul>
<p>We have built an abstraction for the multidimensional array, which makes the
current framework more or less on par with the capabilities of numpy. But in
order to train neural networks we need to recover the most likely parameters
that characterize the underlying generating distribution:</p>
<pre><code>    θ̂ ∈ argmin ℒ(Θ)
      = argmin -P(y1|x1,...yn|xn;Θ)
      = argmin -ΠP(y^i|x^i;Θ) [assuming (X^i,Y^i)~iid]
      = argmin -logΠP(y^i|x^i;Θ) [log monotonicity]
      = argmin -ΣlogP(y^i|x^i;Θ) [log laws]

where argmin is implemented iteratively via gradient descent:
    θ^(t+1) := θ^t - α∇ℒ(Θ)
             = θ^t - α∇-ΣlogP(y^i|x^i;Θ)

and so in the case of binary classification with logistic regression:
    P: ℝ^d -&gt; [0,1]
    P(y^i|x^i;θ={w,b}) := ŷ^y (1-ŷ)^(1-y) = σ(wᵀx)^y [1-σ(wᵀx)]^(1-y)
==&gt; θ̂ ∈ argmin -Σlog[σ(wᵀx)^y [1-σ(wᵀx)]^(1-y)] [by def]
      = argmin ylogσ(wᵀx) - (1-y)log[1-σ(wᵀx)]
==&gt; θ^(t+1) := θ^t - α∇[ ylogσ(wᵀx) - (1-y)log[1-σ(wᵀx)] ]

    ∂ℒ/∂wi = ∂/wi [ ylogσ(wᵀx) - (1-y)log[1-σ(wᵀx)] ]
            = - ∂/wi[ylogσ(wᵀx)] + ∂/∂wi[(1-y)log[1-σ(wᵀx)]] [∂ linear]
            = -y/σ(wᵀx)*∂/wi[σ(wᵀx)] + -(1-y)/[1-σ(wᵀx)]*∂/∂wi[1-σ(wᵀx)] [chain rule]
            = ∂/wi[σ(wᵀx)] * -[y/σ(wᵀx) - (1-y)/[1-σ(wᵀx)]]
            = TODO: ...
            = [σ(wᵀx)-y]xi

and in the case of multiclass classification with softmax regression:
    P: ℝ^d -&gt; [0,1]^k
    P(y^i|x^i) = softmax(Wx), where W ∈ ℝ^{kxd}, x ∈ ℝ^d : x_0 = 1
==&gt; θ̂ ∈ argmin -Σlog[softmax(Wx)] [by def]
==&gt; θ^(t+1) := θ^t - α∇[ -Σlog[softmax(Wx)] ]

and in the case of multiclass classification with neural networks:
    P: ℝ^d -&gt; [0,1]^k
    P(x;θ={w,b}) := (softmax ◦ W_L ◦ (φ ◦ W_{L-1}) ◦ ... ◦ (φ ◦ W_1))(x)
==&gt; θ̂ ∈ argmin -Σlog[(softmax ◦ W_L ◦ (φ ◦ W_{L-1}) ◦ ... ◦ (φ ◦ W_1))(x)] [by def]
==&gt; θ^(t+1) := θ^t - α∇[-Σlog[(softmax ◦ W_L ◦ (φ ◦ W_{L-1}) ◦ ... ◦ (φ ◦ W_1))(x)]]
</code></pre>
<p>but since |θ| where θ={w,b} are reaching the billions and trillions, deriving
the gradient of the loss function with respect to weights and biases becomes
intractable. So in part 2 and part 3 we will add autodifferentiation and
gradient descent support to our tensor library enabling the training of deep
neural networks where deriving the gradient and optimizing the network is
abstracted for the user with a loss.backward() and an optim.step(). The framework
will be able to interpret the following training loop for the FFN from above:</p>
<pre><code># *********************TRAINING LOOP*********************
losses, steps = [], []
for step in range(100): #200000:
    # 1. forward
    # minibatch: X_NT -&gt; X_BT
    i_B = torch.randint(0, X_NT.shape[0], (B,))
    X_BT, Y_B = X_NT[i_B], Y_N[i_B]

    # embed: X_BT -&gt; X_BTE
    X_BTE = C_VE[X_BT] # embed the B examples with T tokens range that span [0..27]
                       # using 0..27 as indices into C_VE
    X_BcTE = X_BTE.view(-1, T * E) #. concat
    X = X_BcTE

    # X_BcTE -&gt; X_BD -&gt; X_BV (y_hat: logits)
    for h in model:
        X = h(X)
    loss = F.cross_entropy(X, Y_B) # 5. picograd.cross_entropy

    # 2. backward
    for layer in model:
        layer.out.retain_grad() # 6 .retain_grad()
    for p in params:
        p.grad = None
    loss.backward()

    # 3. update
    for p in params:
        p.data += -0.01 * p.grad

    steps.append(step)
    losses.append(loss.log10().item())
    if step % 10000 == 0:
        print(f"step: {step}/{200000}, loss {loss.item()}")

plt.plot(steps, losses)
</code></pre>
<h2 id="part-2-gradient-descent-modelbackward-optstep"><a class="header" href="#part-2-gradient-descent-modelbackward-optstep">Part 2: gradient descent: <code>model.backward()</code>, <code>opt.step()</code></a></h2>
<p>2a — Derivative: approximation via local linearization
2b — Derivative rules: backward methods
2c — Automatic differentiation: calculus on computational graph
2d — Advanced AD: jacobian-vector and vector-jacobian products
2e — Gradient descent: non-convex optimization</p>
<h2 id="2a--derivative-approximation-via-local-linearization"><a class="header" href="#2a--derivative-approximation-via-local-linearization">2a — Derivative: approximation via local linearization</a></h2>
<p>The first place to start is to clarify and disambiguate the notion of a derivative
in order to generalize the denotation we use for higher dimensions to hold a more
precise semantics. At the end of part 2a you will have a clear understanding
why the gradient and jacobian are defined the way they are, and how what most
people refer to as "gradients" with pytorch are really jacobians.</p>
<p>By redefining the derivative as a linear operator L defined on some vector space
V that you apply to a change in input to obtain a change in output (sometimes
referred to as the Frechét derivative), it becomes very clear why the gradient
and jacobian are defined the way they are. Clarity that is not achievable when
defining them by fiat. (i.e. "let's <em>just</em> rearrange the partials in a vector/matrix").</p>
<p>Let's begin by recalling that for some f: ℝ -&gt; ℝ the difference of a function at
a point x and x+δx is f(x+δx)-f(x) = δf which can be approximated by f'(x)δx plus
some error terms. That is,</p>
<pre><code>    f(x+δx)-f(x) = δf
                 = f'(x)δx + o(δx)
                 ≈ f'(x)δx

and if we take the difference to be an infinitesimal then it becomes a differential
    f(x+dx)-f(x) = df
                 = f'(x)dx
</code></pre>
<p>and more generally we define the derivative <em>as</em> the linear operator L on vector
space V which you apply to a change in input in order to obtain the change in
output. That is, Δout = (lin.op)[Δin]. which generalizes to higher dimensions.
While defining f'x ≜ df/dx is legal when f: ℝ -&gt; ℝ, it's usually not clear what
df/dx means in higher dimensions, since /: ℝ^n, R^m -&gt; ? is not defined.</p>
<h2 id="f-ℝd---ℝ"><a class="header" href="#f-ℝd---ℝ">f: ℝ^d -&gt; ℝ</a></h2>
<p>Consider f'(x) when f: ℝ^d -&gt; ℝ. Then, given that dx ∈ ℝ and df ∈ ℝ^d, the linear
operator f'(x) <em>must</em> (formalized by the Riecz-Frchét Representation Theorem) be
the dot product with some column vector, (multiplication with the column vector
transposed). Let's call this column vector the "gradient" of f, where the
component-wise definition with indices is:</p>
<pre><code>    ∇f ∈ ℝ^d, f'(x) = ∇fᵀx
    ∇f_i ≜ ∂f/∂x_i ∀ i ∈ [0..d]
and all together, we use f'(x) to linearize df:
    f(x+dx)-f(x) = df
                 = f'(x)dx
                 = ∇fᵀdx
</code></pre>
<h2 id="f-ℝm---ℝn"><a class="header" href="#f-ℝm---ℝn">f: ℝ^m -&gt; ℝ^n</a></h2>
<p>Moving on, consider f'(x) where f: ℝ^m -&gt; ℝ^n. Then given that dx ∈ ℝ^m and
df ∈ ℝ^n, the linear operator f'(x) <em>must</em> (formalized by the Riecz-Frchét
Representation Theorem once again) expressable as a matrix multiplication with
some matrix A ∈ ℝ^(mxn). Let's call this matrix the "jacobian", where the
component-wise definition with indices is the following:</p>
<pre><code>    Jf ∈ ℝ^(nxm), f'(x) = Jdx
    Jf_ij ≜ ∂fi/∂xj ∀ i ∈ [0..n], j ∈ [0..m]
and all together, we use f'(x) to linearize df:
    f(x+dx)-f(x) = df
                 = f'(x)dx
                 = Jdx
</code></pre>
<h2 id="f-ℝm-ℝn---ℝp-f-ℝnxm-ℝmxp---ℝnxp"><a class="header" href="#f-ℝm-ℝn---ℝp-f-ℝnxm-ℝmxp---ℝnxp">f: ℝ^m, ℝ^n -&gt; ℝ^p [f: ℝ^(nxm), ℝ^(mxp) -&gt; ℝ^(nxp)]</a></h2>
<p>For example, given f: ℝ^m -&gt; ℝ^n, f(x) := Ax</p>
<pre><code>==&gt; df = f(x+dx)-f(x)
       = (Ax+dx)-Ax
       = Adx
==&gt; f'(x) = A = J
</code></pre>
<p>While it's very elegant that the derivative of Ax turns out to be A, training
neural networks requires that is A is not just constant, but variable with
respect to f. To make the deep learning context clearer, we will use W instead
of A:</p>
<pre><code>  fi(W,x) ≜ ΣWij*xj
    f(x) := [f1(x), f2(x), ..., fn(x)]
but we can just promote x to be a matrix too. Finally, consider f'(x) when
f: ℝ^(nxm), ℝ^(mxp) -&gt; ℝ^(nxp)

ℒ: ℝ^d0 -&gt; ℝ ∋ ℒ(Z) where Z := X @ Y
==&gt; dℒ = dℒ/dZ : dZ                             [by Riecz. Representation Theorem]
       = dℒ/dZ : (dX@Y + X@dY)                  [by product rule on dZ]
       = dℒ/dZ : (dX@Y) + dℒ/dZ : (X@dY)        [: linear]
       = (dℒ/dZ@Yᵀ) : dX + (Xᵀ@dℒ/dZ) : dY      [by TODO...circularity of trace]                              
==&gt; dℒ/dX = dℒ/dZ@Yᵀ ∧ dℒ/dY = Xᵀ@dℒ/dZ
</code></pre>
<p>Stepping back, defining the derivative this way generalizes the rise over run with
scalars, the gradient with vectors, and the jacobian with matrices into the same
notion: they are the linear operators defined on vector spaces that you apply to
a change in input in order to obtain the change in output, and the types/shapes
are defined the way they are out of necessity, rather than by fiat. Moreover,
this also sharpens the semantics of the denotation in higher dimensions given
that df/dx is often ill-defined. i.e. while df/dx makes sense when df, dx ∈ ℝ^n,
dividing tensors with mismatched shapes becomes nonsensical.</p>
<p>This also means that this definition extends to any vector space where a linear
operator is defined, for instance, the space of matrices (vectorization of
matrices), and the space of functions (calculus of variations).</p>
<p>While training deep neural networks do not involve functions that map
matrices to matrices or functions to functions, defined this way it's clear
that many of the "grads" we take in pytorch are actually jacobians that all
compose into one single gradient for the entire expression graph which represents
some function f: ℝ^n -&gt; ℝ.</p>
<h2 id="2b--derivative-rules-backward-methods"><a class="header" href="#2b--derivative-rules-backward-methods">2b — Derivative rules: backward methods</a></h2>
<p>defining rules for any f: V-&gt;V</p>
<pre><code>Sum rule
--------
f: V -&gt; V
f(x) := g(x) + h(x)
==&gt;      df = dg + dh
    f'(x)dx = g'(x)dx + h'(x)dx   [by def]
      f'(x) = g'(x) + h'(x)       [/ dx]

Product rule
------------
f: V -&gt; V
f(x) := g(x)h(x)
==&gt; df = f(x+dx)-f(x)
       = g(x+dx)h(x+dx)-g(x)h(x)                        [by def of f]
       = [g(x)+g'(x)dx][h(x)+h'(x)dx]-g(x)h(x)          [by def of i(x+dx)=i(x)+i'(x)]
       = g(x)h(x)                                       [expand product]
         + g(x)h'(x)dx
         + h(x)g'(x)dx
         + g'(x)h'(x)(dx)^2
       - g(x)h(x)
       = g(x)h'(x)dx + h(x)g'(x)dx + g'(x)h'(x)(dx)^2   [g(x)h(x) cancels]
       = g(x)h'(x)dx + h(x)g'(x)dx                      [g'(x)h'(x)(dx)^2 -&gt; 0]
       = [g(x)h'(x) + h(x)g'(x)]dx
       = f'(x)dx

Chain rule
----------
f: V -&gt; V
f(x) := g[h(x)]
==&gt; df = f(x+dx)-f(x)
       = g[h(x+dx)]-g[h(x)]
       = g[h(x)+h'(x)dx]-g[h(x)]                  [by def of h(x+dx)=h(x)+h'(x)]
       = g'[h(x)]h'(x)dx                          [g(x+dx)-g(x) = dg = g'(x)dx
                                                   where x=h'(x)dx]
</code></pre>
<h2 id="2c--automatic-differentiation-calculus-on-computational-graph"><a class="header" href="#2c--automatic-differentiation-calculus-on-computational-graph">2c — Automatic differentiation: calculus on computational graph</a></h2>
<p>Recall that we need to evaluate the gradient of a neural network's loss
function in order to train it with parameter estimation. The method used to
compute derivatives is automatic differentiation — an algorithmic technique as
opposed to symbolic techniques that are inefficient or numeric ones that are
inaccurate. Given some expression graph that represents a neural network
f: ℝ^d_0 -&gt; ℝ, autodiff will recursively apply the chain rule for each
subexpression h: ℝ^d_i -&gt; ℝ^d_j.</p>
<pre><code>c = a*b (1)
d = e+f (2)
g = c*d (3)

 .-.
( a &lt;---+
 `-'    |   * .-. (1)
 .-.    +----( c &lt;---+
( b )&lt;--+     `-'    |
 `-'                 |    * .-. (3)
 .-.                 +-----( g )
( e )&lt;--+   / .-. (2)|      `-'
 `-'    +----( d )&lt;--+
 .-.    |     `-'
( f )&lt;--+
 `-'
Fig 4. expression graph
</code></pre>
<p>Notice in figure 4 that even though evaluation of the expression is denoted from
left to right, the edges of the graph form a directed acyclic graph (tree) rooted
at the output of the expression which simplifies graph modification.</p>
<p>Interpreting each step of computation (1, 2, 3) is referred to as the
"forward pass" and constructs the graph in memory which either creates a new tree
amongst a forest (2) or roots the forest into a single tree (3).</p>
<p>Evaluating the derivative of the output with respect to all inputs is referred
to as the "backward pass" — as opposed to "second forward pass" because deep
learning frameworks implement reverse-mode differentiation as opposed to
forward-mode.</p>
<p>The difference between forward-mode and reverse-mode differentiation is the
direction you step through the graph which can end up influencing speed depending
on the shape of the function. Stepping from inputs to outputs is considered a
forward traversal, and the opposite is considered to be a backward traversal.</p>
<p>For instance, consider the applicaiton of the chain rule when evaluating the
f'(x) for f(x) := cos(x^2). Which subexpression do you derive first? Evaluating
d/dx[cos(x)] = sinx first is reverse mode, and evaluating d/dx[x^2] = 2x first
is forward mode. With neural networks, most expression graphs represent functions
of the form f: ℝ^n -&gt; ℝ, and so the default traversal for all deep learning
frameworks is reverse mode.</p>
<p>However, regardless of the order of traversal, the effective computation is the
same: a summation over all possible products of paths. The base case starts
with ∂f/∂f = 1, propagating the local dout/din, and recursively applies the chain
rule for each subexpression. This is why reverse-mode differentiation is also
referred to as "backpropagation". Stepping through the backward for the same
function f as above where d denotes the global derivative and ∂ denotes the local
derivative (overloading ∂'s classic usage for partial derivative)</p>
<pre><code>---forward
c = a*b (1)
d = e+f (2)
g = c*d (3)

---backward
∂g/∂g = 1 [base case] (4)

dg/dc = ∂g/∂g*∂g/∂c           [chain rule]
      = ∂g/∂g*d               [product rule]
      = d                     [∂g/∂g cached] (5)

dg/dd = ∂g/∂g*∂g/∂d           [chain rule]
      = ∂g/∂g*c               [product rule]
      = c                     [∂g/∂g cached] (6)

dg/da = ∂g/∂g*∂g/∂c*∂c/∂a     [chain rule]
      = ∂g/∂g*∂g/∂c*b         [product rule]
      = d*b                   [∂g/∂g*∂g/∂c cached] (7)

dg/db = ∂g/∂g*∂g/∂c*∂c/∂b     [chain rule]
      = ∂g/∂g*∂g/∂c*a         [product rule]
      = d*a                   [∂g/∂g*∂g/∂c cached] (8)

dg/de = ∂g/∂g*∂g/∂d*∂d/∂e     [chain rule]
      = ∂g/∂g*∂g/∂d*1         [sum rule]
      = c                     [∂g/∂g*∂g/∂d cached] (9)

dg/df = ∂g/∂g*∂g/∂d*∂d/∂f     [chain rule]
      = ∂g/∂g*∂g/∂d*1         [sum rule]
      = c                     [∂g/∂g*∂g/∂d cached] (10)

 .-. (7)
( a &lt;---+
 `-'    |   * .-. (1) (5)
 .-.    +----( c &lt;---+
( b )&lt;--+     `-'    |
 `-' (8)             |    * .-. (4)
 .-. (9)             +-----( g )
( e )&lt;--+   + .-. (2)|(6)   `-'
 `-'    +----( d )&lt;--+
 .-.    |     `-'
( f )&lt;--+
 `-'(10)
</code></pre>
<p>What's delightful from a user experience point of view is that autodifferentiation
allows the user to implicitly construct the expression graph by specifying the
forward pass, and the backward pass is abstracted away with an output.backward().</p>
<p>TODO: show expression graph for a neural network.
TODO: let's now revisit the definition of the derivative (2b) and rederive a few derivative rules (2c)</p>
<ul>
<li>+(tensor, tensor)</li>
<li>@(tensor, tensor)</li>
<li>tanh(tensor)</li>
</ul>
<h2 id="2d--advanced-ad-jacobian-vector-and-vector-jacobian-products"><a class="header" href="#2d--advanced-ad-jacobian-vector-and-vector-jacobian-products">2d — Advanced AD: jacobian-vector and vector-jacobian products</a></h2>
<h2 id="2e--gradient-descent-non-convex-optimization"><a class="header" href="#2e--gradient-descent-non-convex-optimization">2e — Gradient descent: non-convex optimization</a></h2>
<p>Finally, recall evaluating the derivative of the computational graph's output
with respect to all inputs is used to implement argmin ℒ(Θ) by iteratively
adjusting θ̂ via gradient descent:
θ^(t+1) := θ^t - α∇ℒ(Θ)
= θ^t - α∇-ΣlogP(y^i|x^i;Θ)</p>
<!-- ## Part 3: attention — a(q,k,v)
Since the discovery of the attention operator in 2017 by (Vaswani et al. 2017),
language, vision, audio and any domain that can be expressed as an autoregressive
sequence of tokens (text/pixels/waves) — that is, an n-dimensional joint distribution:
  p(X_1=x^1,...,X_n=x^m;θ) = Πp(x^i|x^<i)
                             i=1..n
are all converging onto a single network architecture: transformers and their
attention variants. Later in part four we'll reproduce Llama using our deep
learning framework, but for now, we'll prototype with a more simpler feed-forward
neural network (FFN). Here is the model definiton and inference loop:



## Part 4: assistant — RLHF
reinforcement learning from human rewards



## Part 5: reasoner — GRPO
reinforcement learning from automated rewards -->
<h1 id="section-2-speed"><a class="header" href="#section-2-speed">Section 2: Speed</a></h1>
<ul>
<li>does the current cpu ops vectorize? godbolt.</li>
</ul>
<h2 id="section-1-references"><a class="header" href="#section-1-references">Section 1 References</a></h2>
<ol start="0">
<li><a href="https://web.stanford.edu/~jurafsky/slp3/">https://web.stanford.edu/~jurafsky/slp3/</a></li>
<li><a href="https://arxiv.org/pdf/1912.01703">https://arxiv.org/pdf/1912.01703</a></li>
<li><a href="http://blog.ezyang.com/2019/05/pytorch-internals/">http://blog.ezyang.com/2019/05/pytorch-internals/</a></li>
<li><a href="https://docs.jax.dev/en/latest/autodidax.html">https://docs.jax.dev/en/latest/autodidax.html</a></li>
<li><a href="https://numpy.org/doc/stable/dev/internals.html">https://numpy.org/doc/stable/dev/internals.html</a></li>
<li><a href="https://numpy.org/doc/stable/reference/arrays.ndarray.html#internal-memory-layout-of-an-ndarray">https://numpy.org/doc/stable/reference/arrays.ndarray.html#internal-memory-layout-of-an-ndarray</a></li>
<li><a href="https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/">https://ocw.mit.edu/courses/18-s096-matrix-calculus-for-machine-learning-and-beyond-january-iap-2023/</a></li>
<li><a href="https://indico.ijclab.in2p3.fr/event/2914/contributions/6483/subcontributions/180/attachments/6060/7185/automl-short.pdf">https://indico.ijclab.in2p3.fr/event/2914/contributions/6483/subcontributions/180/attachments/6060/7185/automl-short.pdf</a></li>
<li><a href="https://blog.x.com/engineering/en_us/topics/infrastructure/2015/autograd-for-torch">https://blog.x.com/engineering/en_us/topics/infrastructure/2015/autograd-for-torch</a></li>
<li><a href="https://openreview.net/pdf?id=BJJsrmfCZ">https://openreview.net/pdf?id=BJJsrmfCZ</a></li>
<li><a href="http://www.incompleteideas.net/book/RLbook2020.pdf">http://www.incompleteideas.net/book/RLbook2020.pdf</a></li>
<li><a href="https://arxiv.org/pdf/2312.16730">https://arxiv.org/pdf/2312.16730</a></li>
<li><a href="https://arxiv.org/abs/2412.05265">https://arxiv.org/abs/2412.05265</a></li>
<li><a href="https://spinningup.openai.com/en/latest/">https://spinningup.openai.com/en/latest/</a></li>
<li><a href="https://rlhfbook.com/">https://rlhfbook.com/</a></li>
</ol>
<h2 id="section-2-references"><a class="header" href="#section-2-references">Section 2 References</a></h2>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-3-pt2"><a class="header" href="#chapter-3-pt2">Chapter 3: pt2</a></h1>
<p><strong>This chapter implements a compiler for neural networks (pytorch2).</strong> This
compiler will translate a pytorch-like API to a triton-like tiling language. If
you prefer to learn "top-down", read chapter 2 followed by chapter 3. If you
prefer to learn "bottom-up", read chapter 3 followed by chapter 2.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-4-tiles"><a class="header" href="#chapter-4-tiles">Chapter 4: tiles</a></h1>
<p><strong>This chapter implements a compiler for a tiling language (triton).</strong> This
compiler will translate a triton-like eDSL to PTX. If you prefer to learn
"top-down", read chapter 2 followed by chapter 3. If you prefer to learn
"bottom-up", read chapter 3 followed by chapter 2.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
