       ,,      ,...    ,,                 ..                ,,..     
     `7MM    .d' ""  `7MM               pd'               `7MM `bq   
       MM    dM`       MM              6P                   MM   YA  
  ,M""bMM   mMMmm ,M""bMM  `7M'   `MF'6M' `7MMpMMMb.   ,M""bMM   `Mb 
,AP    MM    MM ,AP    MM    `VA ,V'  MN    MM    MM ,AP    MM    8M 
8MI    MM    MM 8MI    MM      XMX    MN    MM    MM 8MI    MM    8M 
`Mb    MM    MM `Mb    MM    ,V' VA.  YM.   MM    MM `Mb    MM   ,M9 
 `Wbmd"MML..JMML.`Wbmd"MML..AM.   .MA. Mb .JMML  JMML.`Wbmd"MML. dM  
                                        Yq.                    .pY   
                                          ``                  ''     
                                                                 
Singularity Systems: Zero to Hero
--------------------------------------------------------------------------------
Syllabus:
  0. au197
  1. dfdx(nd) <-- HERE
    Part 0: non-linear parametric models — nn.Linear(), nn.ReLU
    Part 1: multidimensional array — model.forward()
    Part 2: backpropagation — model.backward()
    Part 3: optimization — opt.step()
  2. brrr
  3. pt2
  4. cloud
Appendix:
  A. nets
  B. c2r5
--------------------------------------------------------------------------------

Welcome to the beginning of a whirlwind tour within the world of deep learning
frameworks! Throughout this journey you will build an interpreter and compiler
for neural networks line by line, from scratch. By the end of the first chapter,
you will have a working implementation of an interpreter that provides an
abstraction for the multidimensional array (like numpy's `ndarray` and pytorch's
`tensor`) with autodifferentiation capabilities to make it *learn*. This
interpreter is capable of training and inferencing the simple FFN (also refferred
to as MLP) following (Bengio et al. 2003), covered in Neural Networks: Zero to
Hero part 2. We will keep the implementation simple using the host language's
CPU support — in the third chapter we will accelerate our computations using SIMD
capability by the CPU and GPU.

---------------------------------------------------------
Part 0: non-linear parametric models — nn.Linear, nn.ReLU
---------------------------------------------------------
Before jumping into the implementation of our deep learning framework's
multidimensional array with autodifferentiation capability, let's review the
mathematical specification of neural networks.

Recall that neural networks are the family of functions that are non-linear
and parametric. The non-linearities that act as feature extractors differentiate
this class from the family of linear functions such as linear and logistic
regression, and the parametric weights and biases differentiate the class from
other non-linear functions such as gaussian processes and kernel methods.

Favoring the mathematical specification over the biological inspiration, neural
networks, reductively put, are a lot of logistic regressions (weighted sums with
sigmoids) stacked together. Recall that the logistic regression model for binary
classification recovers the bernouilli distribution
    P(Y=c|X=x;θ) ~ Ber(p), c={0,1} by assuming
    p:= f(x;θ), f: ℝ^d -> [0,1]
    P(Y=1|X=x;θ) := σ(θᵀx)
==> P(Y=0|X=x;θ) = 1 - σ(θᵀx) [by total law of prob]

and in continuous form:
P(Y=c|X=x) = p^c(1-p)^(1-c)
           = σ(θᵀx)^c [1- σ(θᵀx)]^(1-c)

We can also extend this model to multi-class classification (producing a joint
probability distribution) by generalizing logistic regression to multinomial
logistic regression by replacing σ(z) with softmax(z):
    σ: ℝ -> [0,1], σ(z) := [1+exp(-x)]^-1
    softmax: ℝ^k -> [0,1]^k, softmax(z_i) := exp(z_i) / Σ exp(z_j)
                                                        (j=1 to k)

where z is referred to as the "logits", and recovers the multinomial distribution
    P(Y=y|X=x;θ) ~ X(?), c={0,1,...,k} by assuming
    p:= f(x;θ), f: ℝ^d -> [0,1]^k
    P(Y=y_i|X=x) := exp(θ_iᵀx + b_i) / Σ exp(θ_jᵀx + b_j) [for all i from 1 to k]
                                       (j=1 to k)
==> P(Y=y|X=x) = softmax(θx+b), where θ ∈ ℝ^{kxd}, b ∈ R^k

Finally, we can extend the multinomial logistic regression by introducing many
intermediary steps of representation learning before producing the final
probability distribution. This effectively lifts the complexity of the motif,
finding a more accurate basis to project the input vectors into, before applying
the final softmax:
    f:  ℝ^d -> [0,1]^k
    f(x;θ) := W_L ◦ (φ ◦ W_{L-1}) ◦ ... ◦ (φ ◦ W_1) ◦ x, where θ={w,b}

and each pair of linearity W and non-linearity φ is a "hidden layer" so
    h^(1) := φ(W^{1}x+b_1)
    h^(2) := φ(W^{2}x+b_2)
    .
    .
    .
    h^(L) := φ(W^{L}x+b_L)

TODO
- bias is the first weight w times x_1 = 1.
- note non-linearities are functions of the form φ: ℝ -> ℝ (they don't produce probability distributions).



------------------------------------------------
Part 1: multidimensional array — model.forward()
------------------------------------------------
Let's start with the multidimensional array and follow pytorch's naming convention
by calling it tensor (and overloading its usage in physics which has invariants
that do not hold in statistical learning). Consider the following product type
for tensor:

prod Tensor {
    ndim: int
    shape: int[]
    stride: int[]
    storage: float[]
}

where ndim is an integer, shape as well as stride are lists of integers, and
finally, storage is list of floats. Defining the tensor this way (rather than
naively using nested arrays) makes the aliasing of underlying data buffers quite
natural, since  physical storage can be logically interpreted using shape and
stride. This is desirable in deep learning since many tensor operations only
involve modifying the shape and stride, thereby updating the "view" of the tensor.
Consider example 1 and 2 which illustrate that operations such as reshape and
transpose fundamentally do not add or remove elements from the tensor — they only
move data around. TODO: not really data. but logical interpretation.

Example 1:

01 02 03   reshape      01 02
04 05 06     ==>        03 04

07 08 09                05 06
10 11 12                07 08

                        09 10
                        11 12

Example 2:

1 2 3    transpose    1 4 7
4 5 6      ==>        2 5 8
7 8 9                 3 6 9

Ops
- uops: neg, exp, log, tanh
- binops: add, sub, mul, div
- reduceops: sum, argmax, softmax, max, min, mean, var, std, logsumexp
- processingops: matmul, conv2d, pool2d, maxpool2d, avgpool2d,
- nnops?: linear, sequential, layernorm, batchnorm, crossentropyloss, nllloss

TODO
- map, zip, reduce.
- broadcasting, indexing, views

TODO: why does numpy-C api only support dimension up to 6?

----------------------------------------------
Part 2: autodifferentiation — model.backward()
----------------------------------------------
  2a — Derivative: approximation via local linearization
  2b — Automatic differentiation: calculus on computational graph

We have built an abstraction for the multidimensional array, which makes the
current framework more or less on par with the capabilities of numpy. But in
order to *train* neural networks we need to add support for autodifferentiation,
an algorithmic technique for taking derivatives of mathematical expressions,
as opposed to a symbolic approach (such as Wolfram Alpha) or a numeric one (like
X: TODO). That is, we will follow the footsteps of Wiltschko's torch-autograd
(influencing PyTorch) and Johnson/Maclaurin's HIPS/autograd (influencing JAX).

2a — Derivative: approximation via local linearization
------------------------------------------------------
The first place to start is to clarify and disambiguate the notion of a derivative
in order to generalize the denotation we use for higher dimensions to hold a more
precise semantics. By redefining the derivative as a linear operator L defined on
some vector space V that you apply to a change in input to obtain a change in
output (sometimes referred to as the Frechét derivative), it becomes very clear
why the gradient and jacobian are defined the way they are. Clarity that is not
achievable when defining them by fiat. (i.e. "let's *just* rearrange the partials
in a vector/matrix").

Let's begin by recalling that for some f: ℝ -> ℝ the difference of a function at
a point x and x+δx is f(x+δx)-f(x) = δf which can be approximated by f'(x)δx plus
some error terms. That is,

f(x+δx)-f(x) = δf
             = f'(x)δx + o(δx)
             ≈ f'(x)δx

and if we take the difference to be an infinitesimal then it becomes a differential

f(x+dx)-f(x) = df
             = f'(x)dx

and more generally we define the derivative *as* the linear operator L on vector
space V which you apply to a change in input in order to obtain the change in
output. That is, Δout = (lin.op)[Δin]. which generalizes to higher dimensions.
While defining f'x ≜ df/dx is legal when f: ℝ -> ℝ, it's usually not clear what
df/dx means in higher dimensions, since /: ℝ^n, R^m -> ? is not defined.

Consider df = f'(x)dx when f: ℝ^d -> ℝ. Then, given that dx ∈ ℝ and df ∈ ℝ^d,
the linear operator f'(x) *must* be the dot product with some column vector,
(multiplication with the column vector transposed). Let's call this column vector
the "gradient" of f and as you can now probably tell, this is *why* the gradient
is defined the way it is.

Moving on, if we consider df = f'(x)dx where f: ℝ^n -> ℝ^m, then given that
dx ∈ ℝ^n and df ∈ ℝ^m, the linear operator f'(x) *must* be expressable as a matrix
multiplication with some matrix A ∈ ℝ^(mxn) since any linear transformation is
a combination of scaling, shearing, rotation or reflection. TODO: intuition/explanation
on why matrices are geometric maps. Let's call this matrix the "jacobian".

few derivations of the rules
- sum rule
- product/quotient rule
- chain rule
-> derivations are trivial when you plug in the definitions. (we are assuming the definition is correct.) drops dgdh's.

    f(x) := cos(x^2)
    sinx first, 2x second is reverse mode (inputs -> outputs)
    2x first, sinx last is forward mode (outputs -> inputs)


Stepping back, defining the derivative this way generalizes the rise over run with
scalars, the gradient with vectors, and the jacobian with matrices into the same
notion: they are the linear operators defined on vector spaces that you apply to
a change in input in order to obtain the change in output, and are defined the
way they are out of necessity, rather than by fiat. This also means that this
definition extends to any vector space where a linear operator is defined, for
instance, the space of matrices (vectorization of matrices), and the space of
functions (calculus of variations).

let's move on to automatic differentiation, the computational technique responsible
for evaluating the derivative.


2b — Automatic differentiation: calculus on computational graph
---------------------------------------------------------------
The same way we moved away from indices while talking about derivatives of
R^n -> R (gradient) and R^n -> R^n (jacobian), we will do the same with neural networks.

h1
h2
.
.
.
hn

f(x) := hn composed hN-1 ... composed h1 composed x



Network design started out with manual feature engineering of $\phi: \mathbb{R}^{d_i} \to \mathbb{R}^{d_{i+1}}$, and evolved into deep learning: the construction of networks with  end-to-end representation learning of all feature extractors $\phi$.

Recall that neural networks

Approach 1: manual
------------------

- e.g closed form derivative of logistic regression
- problem 1: way to labor intensive for deep neural networks,
    especially given regime of bitter lesson scaling laws large language models

Approach 2: symbolic
--------------------
- problem 2: expression swell? as computational graph is flattened??

Approach 3: numeric
-------------------




Approach 4: algorithmic
-----------------------
TODO: jump from indices -> matrices

ΣΠw
p1*p1*p1*p1 + p2*p2*p2*p2 + ... pn*pn*pn*pn
sum over all possible products for the output in the graph

since computational graphs for neural networks look like many inputs to single output (loss),
then reverse mode is more efficient.

if we needed to evaluated one input for many outputs, forward mode is more efficient.



Forward Mode.
Reverse Mode (Backpropagation).





---------------------------------
Part 3: optimization — opt.step()
---------------------------------















References:
[0]: https://cs229.stanford.edu/main_notes.pdf
[1]: https://web.stanford.edu/~jurafsky/slp3/

[2]: https://numpy.org/doc/stable/dev/internals.html
[3]: https://numpy.org/doc/stable/dev/internals.code-explanations.html
[4]: https://numpy.org/doc/stable/reference/arrays.ndarray.html#internal-memory-layout-of-an-ndarray
[5]: https://explained.ai/matrix-calculus/
[6]: https://cs231n.github.io/optimization-2/
[7]: https://colah.github.io/posts/2015-08-Backprop/
[8]: http://neuralnetworksanddeeplearning.com/chap2.html
[9]: https://gdalle.github.io/AutodiffTutorial/

[10]: http://blog.ezyang.com/2019/05/pytorch-internals/
[11]: https://arxiv.org/pdf/1912.01703
[12]: https://docs.jax.dev/en/latest/autodidax.html