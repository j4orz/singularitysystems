od       ,,      ,...    ,,                 ..                ,,..     
     `7MM    .d' ""  `7MM               pd'               `7MM `bq   
       MM    dM`       MM              6P                   MM   YA  
  ,M""bMM   mMMmm ,M""bMM  `7M'   `MF'6M' `7MMpMMMb.   ,M""bMM   `Mb 
,AP    MM    MM ,AP    MM    `VA ,V'  MN    MM    MM ,AP    MM    8M 
8MI    MM    MM 8MI    MM      XMX    MN    MM    MM 8MI    MM    8M 
`Mb    MM    MM `Mb    MM    ,V' VA.  YM.   MM    MM `Mb    MM   ,M9 
 `Wbmd"MML..JMML.`Wbmd"MML..AM.   .MA. Mb .JMML  JMML.`Wbmd"MML. dM  
                                        Yq.                    .pY   
                                          ``                  ''     
                                                                 
Singularity Systems: Zero to Hero
--------------------------------------------------------------------------------
Syllabus:
  0. au197
  1. dfdx(nd) <-- HERE
    Part 0: non-linear parametric models — nn.Linear(), nn.ReLU
    Part 1: multidimensional array — model.forward()
    Part 2: gradient descent — loss.backward(), opt.step()
    Part 3: attention — a(q,k,v)
    Part 4: thought — ??
  2. brrr
  3. pt2
  4. datacenter
Appendix:
  A. c2r5
--------------------------------------------------------------------------------

Welcome to the beginning of a whirlwind tour within the world of deep learning
frameworks! Throughout this journey you will build an interpreter and compiler
for llama and r1 line by line, from scratch. By the end of the first chapter,
you will have a working implementation of the multidimensional array with
autodifferentiation capability capable of interpreting a ffn, llama, and r1.
In the next chapter we will accelerate all three models using GPUs.





---------------------------------------------------------
Part 0: non-linear parametric models — nn.Linear, nn.ReLU
---------------------------------------------------------
Before jumping into the implementation of our deep learning framework's
multidimensional array with autodifferentiation capability, let's review the
mathematics of neural networks. We will incrementally construct a family of
functions from logistic regression, multiclass regression, to neural networks
all for the classification setting where Y⊆ℕ to prepare for part 4 and part 5
where the pretrained models for llama and r1 are neural language models.

Recall that the logistic regression model for binary classification recovers the
bernouilli distribution:
    P: ℝ^d -> [0,1] by assuming
    P(Y=1|X=x;θ={w,b}) := σ(wᵀx) ==> P(Y=0|X=x;θ) = 1 - σ(wᵀx)
==> P(Y=y|X=x;θ) = ŷ^y (1-ŷ)^(1-y) [continuous form]
                 = σ(wᵀx)^y [1-σ(wᵀx)]^(1-y)

+---+             
| x1+---+         
| x2+--+|w_0      
|   |  ||         
|   |w_1|         
| . |  |+---->-+. 
| . |  +-->(wᵀ |σ)
| . |  +--->`--+' 
|   |  |          
|   |  |w_d       
|   |  |          
| xd+--+          
+---+
Fig 1. logistic regression

where we can omit the bias by prepending column vector x with x_0 := 1, using
using w_0 as b. From now on we will implicitly assume that the hyperparameter
d is d_old+1 so there is an extra row for the bias terms. And since we are
training from a dataset D={(x_i, y_i)} (i=1..n), we can evaluate the function
P: ℝ^d -> [0,1], P(x^i;θ={w,b}) := σ(wᵀx^(i)) for all n input-output pairs with
a single matrix vector multiplication:
    ŷ = σ(Xw, dim=0)
where:
    X ∈ ℝ^{nxd}
    w ∈ ℝ^d

We can also extend this model to multi-class classification by generalizing
logistic regression to softmax regression by replacing σ(z) with softmax(z):
    σ: ℝ -> [0,1], σ(z) := 1/[1+exp(-z)]
    softmax: ℝ^k -> [0,1]^k, softmax(z_i) := exp(z_i) / Σ exp(z_j)
                                                        j=1..k

recalling that 1. ∫softmax(z) = 1 and 2. that z is referred to as the logits
since sigmoid and softmax map ℝ to log odds (ln[p/(1-p)]). Generalizing sigmoid
to softmax allows us to recover the multinomial distribution:
    P: ℝ^d -> [0,1]^k by assuming
    P(Y=i|X=x) = exp(w_iᵀx) / Σ exp(w_jᵀx)
                              j=1..k
==> P(Y=y|X=x) = softmax(Wx), where W ∈ ℝ^{kxd}, x ∈ ℝ^d : x_0 = 1
                                                                                                                                       
+---++---------------++----+                          +------------------------+
| x1||               ||w1ᵀσ|                          |exp(w1ᵀx) / Σ exp(w_jᵀx)|
| x2||               ||w2ᵀσ|                          |exp(w2ᵀx) / Σ exp(w_jᵀx)|
|   ||               ||    |                          |                        |
|   ||               ||    |                          |                        |
| . ||               || .  |    +----------------+    | .                      |
| . ||       W       || .  |--->|g(z):=softmax(z)|--->| .                      |
| . ||               || .  |    +----------------+    | .                      |
|   ||               ||    |                          |                        |
|   ||               ||    |                          |                        |
|   ||               ||    |                          |                        |
| xd||               ||wdᵀσ|                          |exp(wdᵀx) / Σ exp(w_jᵀx)|
+---++---------------++----+                          +------------------------+
Fig 2. softmax regression

And to evaluate P: ℝ^d -> [0,1]^k, P(x^i;θ) := softmax(Wx) for all n input-output
pairs with a single matrix multiplication:
    ŷ = softmax(XW, dim=0)
where:
    X ∈ ℝ^{nxd}
    W ∈ ℝ^{dxk}

Finally, we can extend the multinomial logistic regression by introducing
intermediate stages of computation to project the representation of inputs
into a basis that is more tractable when mapping to a distribution:
    P: ℝ^d -> [0,1]^k
    P(x;θ={w,b}) := softmax ◦ W_L ◦ (φ ◦ W_{L-1}) ◦ ... ◦ (φ ◦ W_1) ◦ x

and each pair of linearity W and non-linearity φ is a "hidden layer" so
    h^(1) := φ(W^1x+b_1)
    h^(2) := φ(W^2h^(1)+b_2)
    .
    .
    .
    h^(L) := φ(W^Lh^(L-1)+b_L)

                                                                               +------------------------+
                                                                               |exp(w1ᵀx) / Σ exp(w_jᵀx)|
                                                                               |exp(w2ᵀx) / Σ exp(w_jᵀx)|
                                                                               |                        |
                      +---++---------------++-------+                          |                        |
                      | a1||               ||φ(w1ᵀσ)|    +----------------+    | .                      |
                      | a2||               ||φ(w2ᵀσ)|--->|g(z):=softmax(z)|--->| .                      |
                      |   ||               ||       |    +----------------+    | .                      |
  +-------------------+---++----+          ||       |                          |                        |
 ++----------------------------+|          || .     |                          |                        |
++--++---------------++-------+||  WL      || .     |                          |                        |
| x1||               ||φ(w1ᵀσ)|||          || .     |                          |exp(wdᵀx) / Σ exp(w_jᵀx)|
| x2||               ||φ(w2ᵀσ)|||          ||       |                          +------------------------+
|   ||               ||       |||  *       ||       |
|   ||               ||       ||| *        ||       |
| . ||               || .     |||*         ||φ(wdᵀσ)|
| . ||       W1      || .     ||+----------++-------+
| . ||               || .     |||
|   ||               ||       |||
|   ||               ||       |||
|   ||               ||       |++
| xd++               ++φ(wdᵀσ)++
+---++---------------++-------+
Fig 3. neural network

Recall that neural networks are the family of functions that are non-linear
and parametric. The non-linearities that act as feature extractors differentiate
this class from the family of linear functions such as linear and logistic
regression, and the parametric weights and biases differentiate the class from
other non-linear functions such as gaussian processes and kernel methods. Favoring
the mathematical specification over the biological inspiration, neural networks,
reductively put, are a lot of logistic regressions (weighted sums with
non-linearities) stacked together.

```
"""
model: Neural Language Models (Bengio et al. 2003) URL: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf

Dimension key:
# windows
B: batch size
T: sequence length

# input/output
V: vocabulary size
E: embedding dimension (E != D in paper)
D: model dimension
"""
import picograd
B, T = 32, 3
V, E, D = 27, 10, 200

# *********************MODEL*********************
class Linear:
  def __init__(self, D_in, D_out, bias=True):
    self.W_DiDo = picograd.randn((D_in, D_out)) * (5/3)/D_in**0.5 # kaiming init (He et al. 2015)
    self.b_Do = picograd.zeros(D_out) if bias else None

  def __call__(self, X_Di):
    self.X_Do = X_Di @ self.W_DiDo
    if self.b_Do is not None:
        self.X_Do += self.b_Do
    self.out = self.X_Do
    return self.X_Do

  def parameters(self):
    return [self.W_DiDo] + ([] if self.b_Do is None else [self.b_Do])

class Tanh:
  def __call__(self, X_BD):
    self.X_BD = picograd.tanh(X_BD)
    # plt.hist(self.X_BD.view(-1).tolist(), 50); # distribution of weights
    # plt.imshow(self.X_BD.abs() > 0.99, cmap='gray', interpolation='nearest') # vanishing gradients
    self.out = self.X_BD
    return self.X_BD
  
  def parameters(self):
    return []

model = [
  Linear(T * E, D, bias=False), Tanh(),
  Linear(D, D, bias=False), Tanh(),
  Linear(D, V, bias=False)
]

C_VE = picograd.randn((V,E))

# *********************INFERENCE LOOP*********************
for _ in range(20): # 20 samples
  output, context = [], [0] * T
  while True:
    X_1T = picograd.tensor([context]) # B=1 for inference, T=3, in [0..27] (set to 0 for init)
    X_1TE = C_VE[X_1T] # using 0..27 as indices into C_VE for each B=1 example of context length T
    print(X_1TE)
    X_1cTE = X_1TE.view(-1, T*E) # B=1, TE
    X = X_1cTE

    for h in model:
      X = h(X)

    y_hat = F.softmax(X, dim=1)

    # sample and autoregressively update context
    token = picograd.multinomial(y_hat, num_samples=1, replacement=True).item()#, generator=g).item()
    context = context[1:] + [token]
    output.append(decode[token])
    if token == 0:
        break
  print(''.join(output))
```





------------------------------------------------
Part 1: multidimensional array — model.forward()
------------------------------------------------
Let's start with the multidimensional array and follow pytorch's `tensor` naming
convention. Consider the following product type for tensor:

struct Tensor {
    ndim: i32
    shape: Vec<usize>
    stride: Vec<usize>
    storage: Vec<f32>
}

where ndim is an integer, shape as well as stride are lists of integers, and
finally, storage is list of floats. Defining the tensor this way (rather than
naively using nested arrays) makes the aliasing of underlying data buffers quite
natural, since  physical storage can be logically interpreted using shape and
stride. This is desirable in deep learning since many tensor operations only
involve modifying the shape and stride, thereby updating the "view" of the tensor.
Consider example 1 and 2 which illustrate that operations such as reshape and
transpose fundamentally do not add or remove elements from the tensor — they only
change the interpretation of data:

Example 1:
01 02 03   reshape      01 02
04 05 06     ==>        03 04

07 08 09                05 06
10 11 12                07 08

                        09 10
                        11 12

Example 2:
1 2 3    transpose    1 4 7
4 5 6      ==>        2 5 8
7 8 9                 3 6 9

TODO: rest of ndarray owl.

We have built an abstraction for the multidimensional array, which makes the
current framework more or less on par with the capabilities of numpy. But in
order to train neural networks we need to recover the most likely parameters
that characterize the underlying generating distribution:
    θ̂ ∈ argmin ℒ(Θ)
      = argmin -P(y1|x1,...yn|xn;Θ)
      = argmin -ΠP(y^i|x^i;Θ) [assuming (X^i,Y^i)~iid]
      = argmin -logΠP(y^i|x^i;Θ) [log monotonicity]
      = argmin -ΣlogP(y^i|x^i;Θ) [log laws]

where argmin is implemented iteratively via gradient descent:
    θ^(t+1) := θ^t - α∇ℒ(Θ)
             = θ^t - α∇-ΣlogP(y^i|x^i;Θ)

and so in the case of binary classification with logistic regression:
    P: ℝ^d -> [0,1]
    P(y^i|x^i;θ={w,b}) := ŷ^y (1-ŷ)^(1-y) = σ(wᵀx)^y [1-σ(wᵀx)]^(1-y)
==> θ̂ ∈ argmin -Σlog[σ(wᵀx)^y [1-σ(wᵀx)]^(1-y)] [by def]
      = argmin ylogσ(wᵀx) - (1-y)log[1-σ(wᵀx)]
==> θ^(t+1) := θ^t - α∇[ ylogσ(wᵀx) - (1-y)log[1-σ(wᵀx)] ]

    ∂ℒ/∂wi = ∂/wi [ ylogσ(wᵀx) - (1-y)log[1-σ(wᵀx)] ]
            = - ∂/wi[ylogσ(wᵀx)] + ∂/∂wi[(1-y)log[1-σ(wᵀx)]] [∂ linear]
            = -y/σ(wᵀx)*∂/wi[σ(wᵀx)] + -(1-y)/[1-σ(wᵀx)]*∂/∂wi[1-σ(wᵀx)] [chain rule]
            = ∂/wi[σ(wᵀx)] * -[y/σ(wᵀx) - (1-y)/[1-σ(wᵀx)]]
            = TODO: ...
            = [σ(wᵀx)-y]xi

and in the case of multiclass classification with softmax regression:
    P: ℝ^d -> [0,1]^k
    P(y^i|x^i) = softmax(Wx), where W ∈ ℝ^{kxd}, x ∈ ℝ^d : x_0 = 1
==> θ̂ ∈ argmin -Σlog[softmax(Wx)] [by def]
==> θ^(t+1) := θ^t - α∇[ -Σlog[softmax(Wx)] ]

and in the case of multiclass classification with neural networks:
    P: ℝ^d -> [0,1]^k
    P(x;θ={w,b}) := softmax ◦ W_L ◦ (φ ◦ W_{L-1}) ◦ ... ◦ (φ ◦ W_1) ◦ x
==> θ̂ ∈ argmin -Σlog[softmax ◦ W_L ◦ (φ ◦ W_{L-1}) ◦ ... ◦ (φ ◦ W_1) ◦ x] [by def]
==> θ^(t+1) := θ^t - α∇[-Σlog[softmax ◦ W_L ◦ (φ ◦ W_{L-1}) ◦ ... ◦ (φ ◦ W_1) ◦ x]]

but since |θ| where θ={w,b} are reaching the billions and trillions, deriving
the gradient of the loss function with respect to weights and biases becomes
intractable. So in part 2 and part 3 we will add autodifferentiation and
gradient descent support to our tensor library enabling the training of deep
neural networks where deriving the gradient and optimizing the network is
abstracted for the user with a loss.backward() and an optim.step(). The framework
will be able to interpret the following training loop for the FFN from above:

```
# *********************TRAINING LOOP*********************
losses, steps = [], []
for step in range(100): #200000:
    # 1. forward
    # minibatch: X_NT -> X_BT
    i_B = torch.randint(0, X_NT.shape[0], (B,))
    X_BT, Y_B = X_NT[i_B], Y_N[i_B]

    # embed: X_BT -> X_BTE
    X_BTE = C_VE[X_BT] # embed the B examples with T tokens range that span [0..27]
                       # using 0..27 as indices into C_VE
    X_BcTE = X_BTE.view(-1, T * E) #. concat
    X = X_BcTE

    # X_BcTE -> X_BD -> X_BV (y_hat: logits)
    for h in model:
        X = h(X)
    loss = F.cross_entropy(X, Y_B) # 5. picograd.cross_entropy

    # 2. backward
    for layer in model:
        layer.out.retain_grad() # 6 .retain_grad()
    for p in params:
        p.grad = None
    loss.backward()

    # 3. update
    for p in params:
        p.data += -0.01 * p.grad

    steps.append(step)
    losses.append(loss.log10().item())
    if step % 10000 == 0:
        print(f"step: {step}/{200000}, loss {loss.item()}")

plt.plot(steps, losses)
```





-------------------------------------------------------
Part 2: gradient descent — model.backward(), opt.step()
-------------------------------------------------------
  2a — Derivative: approximation via local linearization
  2b — Automatic differentiation: calculus on computational graph
  2c — Gradient descent: non-convex optimization

2a — Derivative: approximation via local linearization
------------------------------------------------------
The first place to start is to clarify and disambiguate the notion of a derivative
in order to generalize the denotation we use for higher dimensions to hold a more
precise semantics. At the end of part 2a you will have a clear understanding
why the gradient and jacobian are defined the way they are, and how what most
people refer to as "gradients" with pytorch are really jacobians.

By redefining the derivative as a linear operator L defined on some vector space
V that you apply to a change in input to obtain a change in output (sometimes
referred to as the Frechét derivative), it becomes very clear why the gradient
and jacobian are defined the way they are. Clarity that is not achievable when
defining them by fiat. (i.e. "let's *just* rearrange the partials in a vector/matrix").

Let's begin by recalling that for some f: ℝ -> ℝ the difference of a function at
a point x and x+δx is f(x+δx)-f(x) = δf which can be approximated by f'(x)δx plus
some error terms. That is,
    f(x+δx)-f(x) = δf
                = f'(x)δx + o(δx)
                ≈ f'(x)δx

and if we take the difference to be an infinitesimal then it becomes a differential
    f(x+dx)-f(x) = df
                = f'(x)dx

and more generally we define the derivative *as* the linear operator L on vector
space V which you apply to a change in input in order to obtain the change in
output. That is, Δout = (lin.op)[Δin]. which generalizes to higher dimensions.
While defining f'x ≜ df/dx is legal when f: ℝ -> ℝ, it's usually not clear what
df/dx means in higher dimensions, since /: ℝ^n, R^m -> ? is not defined.

Consider df = f'(x)dx when f: ℝ^d -> ℝ. Then, given that dx ∈ ℝ and df ∈ ℝ^d,
the linear operator f'(x) *must* be the dot product with some column vector,
(multiplication with the column vector transposed). Let's call this column vector
the "gradient" of f and as you can now probably tell, this is *why* the gradient
is defined the way it is.

Moving on, if we consider df = f'(x)dx where f: ℝ^n -> ℝ^m, then given that
dx ∈ ℝ^n and df ∈ ℝ^m, the linear operator f'(x) *must* be expressable as a matrix
multiplication with some matrix A ∈ ℝ^(mxn) since any linear transformation is
a combination of scaling, shearing, rotation or reflection. Let's call this matrix
the "jacobian".

Stepping back, defining the derivative this way generalizes the rise over run with
scalars, the gradient with vectors, and the jacobian with matrices into the same
notion: they are the linear operators defined on vector spaces that you apply to
a change in input in order to obtain the change in output, and are defined the
way they are out of necessity, rather than by fiat. This also means that this
definition extends to any vector space where a linear operator is defined, for
instance, the space of matrices (vectorization of matrices), and the space of
functions (calculus of variations).

While training deep neural networks does not involve functions that map
matrices to matrices or functions to functions, defined this way it's clear
that many of the "grads" we take in pytorch are actually jacobians that all
compose into one gradient for the entire expression graph.

2b — Automatic differentiation: calculus on computational graph
---------------------------------------------------------------
Recall that we need to evaluate the gradient of a neural network's loss
function in order to train it with parameter estimation. The method used to
compute derivatives is automatic differentiation — an algorithmic technique as
opposed to symbolic techniques that are inefficient or numeric ones that are
inaccurate. Given some expression graph that represents a neural network
f: ℝ^d_0 -> ℝ, autodiff will recursively apply the chain rule for each
subexpression h: ℝ^d_i -> ℝ^d_j.

c = a*b (1)
d = e+f (2)
g = c*d (3)

 .-.
( a <---+
 `-'    |   * .-. (1)
 .-.    +----( c <---+
( b )<--+     `-'    |
 `-'                 |    * .-. (3)
 .-.                 +-----( g )
( e )<--+   / .-. (2)|      `-'
 `-'    +----( d )<--+
 .-.    |     `-'
( f )<--+
 `-'
Fig 4. expression graph

Notice in figure 4 that even though evaluation of the expression is denoted from
left to right, the edges of the graph form a directed acyclic graph (tree) rooted
at the output of the expression which simplifies graph modification.

Interpreting each step of computation (1, 2, 3) is referred to as the
"forward pass" and constructs the graph in memory which either creates a new tree
amongst a forest (2) or roots the forest into a single tree (3).

Evaluating the derivative of the output with respect to all inputs is referred
to as the "backward pass" — as opposed to "second forward pass" because deep
learning frameworks implement reverse-mode differentiation as opposed to
forward-mode — and recursively applies the chain rule to evaluate all partials.

The difference between forward-mode and reverse-mode differentiation is the
direction you step through the graph which ends up influencing speed — stepping
inside out is traversing the graph forwards, and stepping outside in is traversing
the graph backwards. Consider the applicaiton of the chain rule when evaluating
the f'(x) for the function f(x) := cos(x^2). Which subexpression do you derive
first? Evaluating d/dx[cos(x)] = sinx first is reverse mode, and evaluating
d/dx[x^2] = 2x first is forward mode.

c = a*b (1)
d = e+f (2)
g = c*d (3)
∂g/∂g = 1 (4)
∂g/∂c = d (5) [product rule]
∂g/∂d = c (6) [product rule]

∂c/∂a = b(7) [product rule]
∂c/∂b = a(8) [product rule]

∂d/e = 1 (9) [sum rule]
∂d/f = 1 (10) [sum rule]


 .-. (7)
( a <---+
 `-'    |   * .-. (1) (5)
 .-.    +----( c <---+
( b )<--+     `-'    |
 `-' (8)             |    * .-. (4)
 .-. (9)             +-----( g )
( e )<--+   / .-. (2)|(6)   `-'
 `-'    +----( d )<--+
 .-.    |     `-'
( f )<--+
 `-'(10)
Fig 4. expression graph

TODO: reverse mode: R^n -> R^m m (number of outputs) forward mode: R^n -> R^m n (number of inputs)
Regardless of whether autodifferentiation is implemented with forward or reverse-mode,
the end computation is the same: we are summing over all possible products of paths.

TODO: autodifferentiation through a graph representing a neural network

2c — Gradient descent: non-convex optimization
----------------------------------------------
nn training (non-convex) TODO:
- initialization random
- initialization to 0 mean and unit variance
- GD, SGD, AdamW, Muon
























----------------------------------
Part 3: attention — a(q,k,v) TODO:
----------------------------------
Since the discovery of the attention operator in 2017 by (Vaswani et al. 2017),
language, vision, audio and any domain that can be expressed as an autoregressive
sequence of tokens (text/pixels/waves) — that is, an n-dimensional joint distribution:
  p(X_1=x^1,...,X_n=x^m;θ) = Πp(x^i|x^<i)
                             i=1..n
are all converging onto a single network architecture: transformers and their
attention variants. Later in part four we'll reproduce Llama using our deep
learning framework, but for now, we'll prototype with a more simpler feed-forward
neural network (FFN). Here is the model definiton and inference loop:





----------------------------
Part 4: thought — TODO:
----------------------------






References
----------
[0]: https://web.stanford.edu/~jurafsky/slp3/
[1]: https://cs229.stanford.edu/main_notes.pdf
[2]: https://www.sscardapane.it/alice-book/
[3]: https://arxiv.org/pdf/1912.01703
[4]: http://blog.ezyang.com/2019/05/pytorch-internals/
[5]: https://docs.jax.dev/en/latest/autodidax.html
[6]: https://numpy.org/doc/stable/dev/internals.html
[7]: https://numpy.org/doc/stable/dev/internals.code-explanations.html
[8]: https://numpy.org/doc/stable/reference/arrays.ndarray.html#internal-memory-layout-of-an-ndarray
[9]: https://cs231n.github.io/optimization-2/
[10]: https://colah.github.io/posts/2015-08-Backprop/
[11]: https://gdalle.github.io/AutodiffTutorial/