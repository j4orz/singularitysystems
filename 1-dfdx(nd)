       ,,      ,...    ,,                 ..                ,,..     
     `7MM    .d' ""  `7MM               pd'               `7MM `bq   
       MM    dM`       MM              6P                   MM   YA  
  ,M""bMM   mMMmm ,M""bMM  `7M'   `MF'6M' `7MMpMMMb.   ,M""bMM   `Mb 
,AP    MM    MM ,AP    MM    `VA ,V'  MN    MM    MM ,AP    MM    8M 
8MI    MM    MM 8MI    MM      XMX    MN    MM    MM 8MI    MM    8M 
`Mb    MM    MM `Mb    MM    ,V' VA.  YM.   MM    MM `Mb    MM   ,M9 
 `Wbmd"MML..JMML.`Wbmd"MML..AM.   .MA. Mb .JMML  JMML.`Wbmd"MML. dM  
                                        Yq.                    .pY   
                                          ``                  ''     
                                                                 
Singularity Systems: Zero to Hero
--------------------------------------------------------------------------------
Syllabus:
  0. au197
  1. dfdx(nd) <-- HERE
    Part 0: non-linear parametric models — nn.Linear(), nn.ReLU
    Part 1: multidimensional array — model.forward()
    Part 2: backpropagation — model.backward()
    Part 3: optimization — opt.step()
    Part 4: v0.1.1: — d̶o̶o̶m̶llama-complete
  2. brrr
  3. pt2
  4. cloud
Appendix:
  A. c2r5
--------------------------------------------------------------------------------

Welcome to the beginning of a whirlwind tour within the world of deep learning
frameworks! Throughout this journey you will build an interpreter and compiler
for neural networks line by line, from scratch. By the end of the first chapter,
you will have a working implementation of an interpreter that provides an
abstraction for the multidimensional array (like numpy's `ndarray` and pytorch's
`tensor`) with autodifferentiation capabilities to make it *learn*. This
interpreter is capable of training and inferencing the simple FFN (also refferred
to as MLP) following (Bengio et al. 2003), covered in Neural Networks: Zero to
Hero part 2. We will keep the implementation simple using the host language's
CPU support — in the second chapter we will accelerate our computations using
SIMD provided by the CPU and GPU.

---------------------------------------------------------
Part 0: non-linear parametric models — nn.Linear, nn.ReLU
---------------------------------------------------------
Before jumping into the implementation of our deep learning framework's
multidimensional array with autodifferentiation capability, let's review the
mathematics of neural networks.

Recall that neural networks are the family of functions that are non-linear
and parametric. The non-linearities that act as feature extractors differentiate
this class from the family of linear functions such as linear and logistic
regression, and the parametric weights and biases differentiate the class from
other non-linear functions such as gaussian processes and kernel methods.

Favoring the mathematical specification over the biological inspiration, neural
networks, reductively put, are a lot of logistic regressions (weighted sums with
sigmoids) stacked together. Recall that the logistic regression model for binary
classification recovers the bernouilli distribution
    P(Y=c|X=x;θ) ~iid Ber(p), c={0,1} by assuming
    p:= f(x;θ), f: ℝ^d -> [0,1]
    P(Y=1|X=x;θ) := σ(θᵀx)
==> P(Y=0|X=x;θ) = 1 - σ(θᵀx) [by total law of prob]
==> P(Y=c|X=x) = p^c(1-p)^(1-c) [continuous form]
               = σ(θᵀx)^c [1- σ(θᵀx)]^(1-c)

We can also extend this model to multi-class classification (producing a joint
probability distribution) by generalizing logistic regression to multinomial
logistic regression by replacing σ(z) with softmax(z):
    σ: ℝ -> [0,1], σ(z) := [1+exp(-x)]^-1
    softmax: ℝ^k -> [0,1]^k, softmax(z_i) := exp(z_i) / Σ exp(z_j)
                                                        j=1..k

where z is referred to as the "logits", and recovers the multinomial distribution
    P(Y=y|X=x;θ) ~iid X(?), c={0,1,...,k} by assuming
    p:= f(x;θ), f: ℝ^d -> [0,1]^k
    P(Y=y_i|X=x) := exp(θ_iᵀx + b_i) / Σ exp(θ_jᵀx + b_j) [for all i from 1 to k]
                                       j=1..k
==> P(Y=y|X=x) = softmax(θx+b), where θ ∈ ℝ^{kxd}, b ∈ R^k

Finally, we can extend the multinomial logistic regression by introducing
intermediate steps of representation learning where the model learns a more
accurate basis to project the input vectors into before applying the final
softmax:
    f:  ℝ^d -> [0,1]^k
    f(x;θ) := W_L ◦ (φ ◦ W_{L-1}) ◦ ... ◦ (φ ◦ W_1) ◦ x, where θ={w,b}

and each pair of linearity W and non-linearity φ is a "hidden layer" so
    h^(1) := φ(W^{1}x+b_1)
    h^(2) := φ(W^{2}h^(1)+b_2)
    .
    .
    .
    h^(L) := φ(W^{L}h^(L-1)+b_L)
* we can omit the biases in the definition for f by prepending column vector
x with x_0 := 1, effectively using the first row of all weight matrices as biases.

Since the discovery of the attention operator in 2017 by (Vaswani et al. 2017),
language, vision, audio and any domain that can be expressed as an autoregressive
sequence of tokens (text/pixels/waves) — that is, an n-dimensional joint distribution:
  p(X_1=x^1,...,X_n=x^m;θ) = Πp(x^i|x^<i)
                             i=1..n
are all converging onto a single network architecture: transformers and their
attention variants. Later in part four we'll reproduce Llama using our deep
learning framework, but for now, we'll prototype with a more simpler feed-forward
neural network (FFN). Here is the model definiton and inference loop:

```
"""
model: Neural Language Models (Bengio et al. 2003) URL: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf

Dimension key:
# windows
B: batch size
T: sequence length

# input/output
V: vocabulary size
E: embedding dimension (E != D in paper)
D: model dimension
"""
import picograd
B, T = 32, 3
V, E, D = 27, 10, 200

# *********************MODEL*********************
class Linear:
  def __init__(self, D_in, D_out, bias=True):
    self.W_DiDo = picograd.randn((D_in, D_out)) * (5/3)/D_in**0.5 # kaiming init (He et al. 2015)
    self.b_Do = picograd.zeros(D_out) if bias else None

  def __call__(self, X_Di):
    self.X_Do = X_Di @ self.W_DiDo
    if self.b_Do is not None:
        self.X_Do += self.b_Do
    self.out = self.X_Do
    return self.X_Do

  def parameters(self):
    return [self.W_DiDo] + ([] if self.b_Do is None else [self.b_Do])

class Tanh:
  def __call__(self, X_BD):
    self.X_BD = picograd.tanh(X_BD)
    # plt.hist(self.X_BD.view(-1).tolist(), 50); # distribution of weights
    # plt.imshow(self.X_BD.abs() > 0.99, cmap='gray', interpolation='nearest') # vanishing gradients
    self.out = self.X_BD
    return self.X_BD
  
  def parameters(self):
    return []

model = [
  Linear(T * E, D, bias=False), Tanh(),
  Linear(D, D, bias=False), Tanh(),
  Linear(D, V, bias=False)
]

C_VE = picograd.randn((V,E))

# *********************INFERENCE LOOP*********************
for _ in range(20): # 20 samples
  output, context = [], [0] * T
  while True:
    X_1T = picograd.tensor([context]) # B=1 for inference, T=3, in [0..27] (set to 0 for init)
    X_1TE = C_VE[X_1T] # using 0..27 as indices into C_VE for each B=1 example of context length T
    print(X_1TE)
    X_1cTE = X_1TE.view(-1, T*E) # B=1, TE
    X = X_1cTE

    for h in model:
      X = h(X)

    y_hat = F.softmax(X, dim=1)

    # sample and autoregressively update context
    token = picograd.multinomial(y_hat, num_samples=1, replacement=True).item()#, generator=g).item()
    context = context[1:] + [token]
    output.append(decode[token])
    if token == 0:
        break
  print(''.join(output))
```

------------------------------------------------
Part 1: multidimensional array — model.forward()
------------------------------------------------
Let's start with the multidimensional array and follow pytorch's naming convention
by calling it tensor (and overloading its usage in physics which has invariants
that do not hold in statistical learning). Consider the following product type
for tensor:

prod Tensor {
    ndim: int
    shape: int[]
    stride: int[]
    storage: float[]
}

where ndim is an integer, shape as well as stride are lists of integers, and
finally, storage is list of floats. Defining the tensor this way (rather than
naively using nested arrays) makes the aliasing of underlying data buffers quite
natural, since  physical storage can be logically interpreted using shape and
stride. This is desirable in deep learning since many tensor operations only
involve modifying the shape and stride, thereby updating the "view" of the tensor.
Consider example 1 and 2 which illustrate that operations such as reshape and
transpose fundamentally do not add or remove elements from the tensor — they only
move data around. TODO: not really data. but logical interpretation.

Example 1:

01 02 03   reshape      01 02
04 05 06     ==>        03 04

07 08 09                05 06
10 11 12                07 08

                        09 10
                        11 12

Example 2:

1 2 3    transpose    1 4 7
4 5 6      ==>        2 5 8
7 8 9                 3 6 9

Ops
- uops: neg, exp, log, tanh
- binops: add, sub, mul, div
- reduceops: sum, argmax, softmax, max, min, mean, var, std, logsumexp
- processingops: matmul, conv2d, pool2d, maxpool2d, avgpool2d,
- nnops?: linear, sequential, layernorm, batchnorm, crossentropyloss, nllloss

TODO
- map, zip, reduce.
- broadcasting, indexing, views

----------------------------------------------
Part 2: autodifferentiation — model.backward()
----------------------------------------------
  2a — Derivative: approximation via local linearization
  2b — Automatic differentiation: calculus on computational graph

We have built an abstraction for the multidimensional array, which makes the
current framework more or less on par with the capabilities of numpy. But in
order to *train* neural networks we need to add support for autodifferentiation,
an algorithmic technique for taking derivatives of mathematical expressions,
as opposed to a symbolic approach (such as Wolfram Alpha) or a numeric one (like
X: TODO). That is, we will follow the footsteps of Wiltschko's torch-autograd
(influencing PyTorch) and Johnson/Maclaurin's HIPS/autograd (influencing JAX).

Recall the definition of a neural network:

Here is the training loop for our FFN:
```
# random.seed(42)
# random.shuffle(words)
# n1, n2 = int(0.8*len(words)), int(0.9*len(words))
X_NT, Y_N = gen_dataset(words)#[:n1])
print(X_NT.shape, Y_N.shape)
# Xdev, Ydev = gen_dataset(words[n1:n2])
# Xte, Yte = gen_dataset(words[n2:])

# 2. training loop
losses, steps = [], []
for step in range(100): #200000:
    # 1. forward
    # minibatch: X_NT -> X_BT
    i_B = torch.randint(0, X_NT.shape[0], (B,))
    X_BT, Y_B = X_NT[i_B], Y_N[i_B]

    # embed: X_BT -> X_BTE
    X_BTE = C_VE[X_BT] # embed the B examples with T tokens range that span [0..27]
                       # using 0..27 as indices into C_VE
    X_BcTE = X_BTE.view(-1, T * E) #. concat
    X = X_BcTE

    # X_BcTE -> X_BD -> X_BV (y_hat: logits)
    for h in model:
        X = h(X)
    loss = F.cross_entropy(X, Y_B) # 5. picograd.cross_entropy

    # 2. backward
    for layer in model:
        layer.out.retain_grad() # 6 .retain_grad()
    for p in params:
        p.grad = None
    loss.backward()

    # 3. update
    for p in params:
        p.data += -0.01 * p.grad

    steps.append(step)
    losses.append(loss.log10().item())
    if step % 10000 == 0:
        print(f"step: {step}/{200000}, loss {loss.item()}")

plt.plot(steps, losses)
```

2a — Derivative: approximation via local linearization
------------------------------------------------------
The first place to start is to clarify and disambiguate the notion of a derivative
in order to generalize the denotation we use for higher dimensions to hold a more
precise semantics. By redefining the derivative as a linear operator L defined on
some vector space V that you apply to a change in input to obtain a change in
output (sometimes referred to as the Frechét derivative), it becomes very clear
why the gradient and jacobian are defined the way they are. Clarity that is not
achievable when defining them by fiat. (i.e. "let's *just* rearrange the partials
in a vector/matrix").

Let's begin by recalling that for some f: ℝ -> ℝ the difference of a function at
a point x and x+δx is f(x+δx)-f(x) = δf which can be approximated by f'(x)δx plus
some error terms. That is,

f(x+δx)-f(x) = δf
             = f'(x)δx + o(δx)
             ≈ f'(x)δx

and if we take the difference to be an infinitesimal then it becomes a differential

f(x+dx)-f(x) = df
             = f'(x)dx

and more generally we define the derivative *as* the linear operator L on vector
space V which you apply to a change in input in order to obtain the change in
output. That is, Δout = (lin.op)[Δin]. which generalizes to higher dimensions.
While defining f'x ≜ df/dx is legal when f: ℝ -> ℝ, it's usually not clear what
df/dx means in higher dimensions, since /: ℝ^n, R^m -> ? is not defined.

Consider df = f'(x)dx when f: ℝ^d -> ℝ. Then, given that dx ∈ ℝ and df ∈ ℝ^d,
the linear operator f'(x) *must* be the dot product with some column vector,
(multiplication with the column vector transposed). Let's call this column vector
the "gradient" of f and as you can now probably tell, this is *why* the gradient
is defined the way it is.

Moving on, if we consider df = f'(x)dx where f: ℝ^n -> ℝ^m, then given that
dx ∈ ℝ^n and df ∈ ℝ^m, the linear operator f'(x) *must* be expressable as a matrix
multiplication with some matrix A ∈ ℝ^(mxn) since any linear transformation is
a combination of scaling, shearing, rotation or reflection. TODO: intuition/explanation
on why matrices are geometric maps. Let's call this matrix the "jacobian".

few derivations of the rules
- sum rule
- product/quotient rule
- chain rule
-> derivations are trivial when you plug in the definitions. (we are assuming the definition is correct.) drops dgdh's.

    f(x) := cos(x^2)
    sinx first, 2x second is reverse mode (inputs -> outputs)
    2x first, sinx last is forward mode (outputs -> inputs)


Stepping back, defining the derivative this way generalizes the rise over run with
scalars, the gradient with vectors, and the jacobian with matrices into the same
notion: they are the linear operators defined on vector spaces that you apply to
a change in input in order to obtain the change in output, and are defined the
way they are out of necessity, rather than by fiat. This also means that this
definition extends to any vector space where a linear operator is defined, for
instance, the space of matrices (vectorization of matrices), and the space of
functions (calculus of variations).

let's move on to automatic differentiation, the computational technique responsible
for evaluating the derivative.


2b — Automatic differentiation: calculus on computational graph
---------------------------------------------------------------

Approach 1: manual
------------------

- e.g closed form derivative of logistic regression
- problem 1: way to labor intensive for deep neural networks,
    especially given regime of bitter lesson scaling laws large language models

Approach 2: symbolic
--------------------
- problem 2: expression swell? as computational graph is flattened??

Approach 3: numeric
-------------------




Approach 4: algorithmic
-----------------------
TODO: jump from indices -> matrices

ΣΠw
p1*p1*p1*p1 + p2*p2*p2*p2 + ... pn*pn*pn*pn
sum over all possible products for the output in the graph

since computational graphs for neural networks look like many inputs to single output (loss),
then reverse mode is more efficient.

if we needed to evaluated one input for many outputs, forward mode is more efficient.



Forward Mode.
Reverse Mode (Backpropagation).















---------------------------------
Part 3: optimization — opt.step()
---------------------------------
---------------------------------
Part 4: d̶o̶o̶m̶ llama-complete
---------------------------------

References:
[0]: https://cs229.stanford.edu/main_notes.pdf
[1]: https://web.stanford.edu/~jurafsky/slp3/

[2]: https://numpy.org/doc/stable/dev/internals.html
[3]: https://numpy.org/doc/stable/dev/internals.code-explanations.html
[4]: https://numpy.org/doc/stable/reference/arrays.ndarray.html#internal-memory-layout-of-an-ndarray
[5]: https://explained.ai/matrix-calculus/
[6]: https://cs231n.github.io/optimization-2/
[7]: https://colah.github.io/posts/2015-08-Backprop/
[8]: http://neuralnetworksanddeeplearning.com/chap2.html
[9]: https://gdalle.github.io/AutodiffTutorial/

[10]: http://blog.ezyang.com/2019/05/pytorch-internals/
[11]: https://arxiv.org/pdf/1912.01703
[12]: https://docs.jax.dev/en/latest/autodidax.html